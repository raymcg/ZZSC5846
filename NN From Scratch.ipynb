{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import random\n",
    "import time\n",
    "\n",
    "# import library to normalize data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "fpath = 'C:\\\\Dropbox\\\\Variance\\\\UNSW\\\\ZZSC5836\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Class and Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "\n",
    "\tdef __init__(self, Topo, Train, Test, MaxTime, Samples, MinPer, learnRate): \n",
    "\t\tself.Top  = Topo  # NN topology [input, hidden, output]\n",
    "\t\tself.Max = MaxTime # max epocs\n",
    "\t\tself.TrainData = Train # This is both X and y, it gets split during the BP_GD function\n",
    "\t\tself.TestData = Test # This is both X and y, it gets split during the BP_GD function\n",
    "\t\tself.NumSamples = Samples\n",
    "\n",
    "\t\tprint(f\"self.NumSamples: {self.NumSamples}\")\n",
    "\n",
    "\t\tself.learn_rate  = learnRate\n",
    "\n",
    "\t\tself.minPerf = MinPer\n",
    "\t\t\n",
    "\t\t#initialize weights ( W1 W2 ) and bias ( b1 b2 ) of the network\n",
    "\t\tnp.random.seed() \n",
    "\t\tself.W1 = np.random.uniform(-0.5, 0.5, (self.Top[0] , self.Top[1]))  \n",
    "\t\t#print(self.W1,  ' self.W1')\n",
    "\t\tself.B1 = np.random.uniform(-0.5,0.5, (1, self.Top[1])  ) # bias first layer\n",
    "\t\t#print(self.B1, ' self.B1')\n",
    "\t\tself.BestB1 = self.B1\n",
    "\t\tself.BestW1 = self.W1 \n",
    "\t\tself.W2 = np.random.uniform(-0.5, 0.5, (self.Top[1] , self.Top[2]))   \n",
    "\t\tself.B2 = np.random.uniform(-0.5,0.5, (1,self.Top[2]))  # bias second layer\n",
    "\t\tself.BestB2 = self.B2\n",
    "\t\tself.BestW2 = self.W2 \n",
    "\t\tself.hidout = np.zeros(self.Top[1] ) # output of first hidden layer\n",
    "\t\tself.out = np.zeros(self.Top[2]) #  output last layer\n",
    "\n",
    "\t\tself.hid_delta = np.zeros(self.Top[1] ) # output of first hidden layer\n",
    "\t\tself.out_delta = np.zeros(self.Top[2]) #  output last layer\n",
    "\n",
    "\tdef sigmoid(self,x):\n",
    "\t\treturn 1 / (1 + np.exp(-x))\n",
    "\n",
    "\t\n",
    "\tdef softmax(self, x):\n",
    "\t\t# Numerically stable with large exponentials\n",
    "\t\texps = np.exp(x - x.max())\n",
    "\t\treturn exps / np.sum(exps, axis=0)\n",
    "\n",
    "\tdef sampleEr(self,actualout):\n",
    "\t\terror = np.subtract(self.out, actualout)\n",
    "\t\tsqerror= np.sum(np.square(error))/self.Top[2] \n",
    "\t\t \n",
    "\t\treturn sqerror\n",
    "\n",
    "\tdef ForwardPass(self, X ): \n",
    "\t\t\"\"\"\n",
    "\t\t\tX: The input data or sample which is passed into the network.\n",
    "\n",
    "\t\t\tz1: Weighted sum of the inputs after passing through the first set of weights (`W1`) and having bias (`B1`) subtracted.\n",
    "\t\t\thidout: Output of the first hidden layer. This is the activation (using sigmoid function) applied on `z1`.\n",
    "\n",
    "\t\t\tz2: Weighted sum of `hidout` after passing through the second set of weights (`W2`) and having bias (`B2`) subtracted.\n",
    "\t\t\tout: The final output of the network. This is the activation (using sigmoid function) applied on `z2`.\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tz1 = X.dot(self.W1) - self.B1  #For the first hidden layer, we multipy our input values(X) by our W1 weights and minus the bias\n",
    "\t\tself.hidout = self.sigmoid(z1) # output of first hidden layer   \n",
    "\n",
    "\t\tz2 = self.hidout.dot(self.W2)  - self.B2 #For the second hidden layer, we multiply the output of the first hidden layer (hidout) by our W2 weights and minus the bias\n",
    "\t\tself.out = self.sigmoid(z2)  # output second hidden layer\n",
    "\n",
    "\tdef BackwardPass(self, input_vec, desired):   \n",
    "\n",
    "\t\t\"\"\"\n",
    "\t\t\tinput_vec: The input data or sample which was passed to the network.  This a single row of the training data.\n",
    "\t\t\tdesired: The actual desired output for the `input_vec`.  Y_actual\n",
    "\t\t\tout: The output produced by the model for this input_vec.  Y_pred\n",
    "\t\t\thidout: The output of the hidden layer for this input. \n",
    "\n",
    "\t\t\tout_delta: The error gradient at the output layer. \n",
    "\t\t\t\t\t\tIt is calculated by taking the difference between the desired output and actual output (y_actual - y_pred)), \n",
    "\t\t\t\t\t\tand multiplying by the derivative of the activation function.\n",
    "\t\t\thid_delta: The error gradient at the hidden layer. \n",
    "\t\t\t\t\t\tIt computes how much the hidden layer contributed to the `out_delta` error after taking into account the weights (`W2`).\n",
    "\t\t\tThe weights (`W1` and `W2`) and biases (`B1` and `B2`) are then updated based on these error gradients.\n",
    "\n",
    "\t\t\"\"\"\n",
    "\t\tout_delta =   (desired - self.out)*(self.out*(1-self.out))  \n",
    "\t\thid_delta = out_delta.dot(self.W2.T) * (self.hidout * (1-self.hidout)) #https://www.tutorialspoint.com/numpy/numpy_dot.htm  https://www.geeksforgeeks.org/numpy-dot-python/\n",
    "  \n",
    "\t\tself.W2+= self.hidout.T.dot(out_delta) * self.learn_rate # Update weights for the second layer (hidden to output)\n",
    "\t\tself.B2+=  (-1 * self.learn_rate * out_delta) # Update bias for the second layer (hidden to output)\n",
    "\n",
    "\t\tself.W1 += (input_vec.T.dot(hid_delta) * self.learn_rate)  # Update weights for the first layer (input to hidden)\n",
    "\t\tself.B1+=  (-1 * self.learn_rate * hid_delta) # Update bias for the first layer (input to hidden)\n",
    "\n",
    "\t\n",
    "\tdef TestNetwork(self, Data, testSize, tolerance):\n",
    "\t\tInput = np.zeros((1, self.Top[0])) # temp hold input\n",
    "\t\tDesired = np.zeros((1, self.Top[2])) \n",
    "\t\tnOutput = np.zeros((1, self.Top[2]))\n",
    "\t\tclasPerf = 0\n",
    "\t\tsse = 0  \n",
    "\n",
    "\t\tpredicted = np.zeros(testSize)\n",
    "\n",
    "\t\tself.W1 = self.BestW1\n",
    "\t\tself.W2 = self.BestW2 #load best knowledge\n",
    "\t\tself.B1 = self.BestB1\n",
    "\t\tself.B2 = self.BestB2 #load best knowledge\n",
    "\n",
    "\t\tfor s in range(0, testSize):\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\tInput  =   Data[s,0:self.Top[0]] \n",
    "\t\t\tDesired =  Data[s,self.Top[0]:] \n",
    "\n",
    "\t\t\tself.ForwardPass(Input) \n",
    "\t\t\tpredicted[s] = self.out\n",
    "\t\t\tsse = sse+ self.sampleEr(Desired)  \n",
    "\n",
    "\n",
    "\t\tactual =  Data[:,self.Top[0]:]   \n",
    "\n",
    "\t\t\t#if(np.isclose(self.out, Desired, atol=erTolerance).any()):\n",
    "\t\t\t\t#clasPerf =  clasPerf +1  \n",
    "\n",
    "\t\t#calculate rsquared using sklearn\n",
    "\t\tr2 = r2_score(actual, predicted)\n",
    "\n",
    "\n",
    "\n",
    "\t\treturn np.sqrt(sse/testSize),   r2, actual, predicted\n",
    "\n",
    "\tdef saveKnowledge(self):\n",
    "\t\tself.BestW1 = self.W1\n",
    "\t\tself.BestW2 = self.W2\n",
    "\t\tself.BestB1 = self.B1\n",
    "\t\tself.BestB2 = self.B2  \n",
    "\n",
    "\n",
    "\tdef BP_GD(self):  \n",
    "\n",
    "\t\t\"\"\"\n",
    "\t\t\tInput: Temporary variable to hold the input part of the current training sample.\n",
    "\t\t\tDesired: Temporary variable to hold the desired output (y_act) part of the current training sample.\n",
    "\t\t\tEr: List to keep track of the Root Mean Square Error (RMSE) over epochs.\n",
    "\t\t\tepoch: Current epoch or iteration number.\n",
    "\t\t\tbestmse: Keeps track of the best (lowest) RMSE achieved so far.\n",
    "\t\t\tbestTrain: Stores the best training performance achieved so far.\n",
    "\t\t\tsse: Sum of squared errors for the current epoch.\n",
    "\n",
    "\t\t\tTop[0]: Represents the number of nodes in the input layer. This would correspond to the number of features or inputs the neural network expects for each sample.\n",
    "\t\t\tTop[1]: Represents the number of nodes in the hidden layer.\n",
    "\t\t\tTop[2]: Represents the number of nodes in the output layer. This would correspond to the number of output values the network produces for each input sample.\n",
    "\t\t\n",
    "\t\t\tFor each training sample, it performs a `ForwardPass` to get the output of the network, followed by a `BackwardPass` to adjust the weights based on the error.\n",
    "\n",
    "\t\t\tAfter each epoch, the Mean Square Error (MSE) is computed and checked against the best MSE so far. If the current MSE is better, the best known weights and biases of the network are updated.\n",
    "\n",
    "\t\t\tThe function returns the RMSE for each epoch, the best RMSE achieved, the best training performance, and the total number of epochs executed.\n",
    "\t\t\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tInput = np.zeros((1, self.Top[0])) # temp hold input\n",
    "\t\tDesired = np.zeros((1, self.Top[2])) \n",
    " \n",
    "\t\tEr = [] \n",
    "\t\tepoch = 0\n",
    "\t\tbestrmse = 10000 # assign a large number in begining to maintain best (lowest RMSE)\n",
    "\t\tbestTrain = 0\n",
    "\t\twhile  epoch < self.Max and bestTrain < self.minPerf : # For each epoch (all training rows)\n",
    "\t\t\tsse = 0\n",
    "\t\t\tfor s in range(0, self.NumSamples): # For each row in the training data\n",
    "\t\t\n",
    "\t\t\t\t# Split out X and y\n",
    "\t\t\t\tInput[:]  =  self.TrainData[s,0:self.Top[0]]  #row s, all cols but the last one (X(i))\n",
    "\t\t\t\tDesired[:]  = self.TrainData[s,self.Top[0]:]  #row s, just the last col (Y(i))\n",
    "\n",
    "\t\t\t\tself.ForwardPass(Input)  \n",
    "\t\t\t\tself.BackwardPass(Input ,Desired)\n",
    "\t\t\t\tsse = sse+ self.sampleEr(Desired) # accumulated the error for each row to get a total for the whole training set for this epoch\n",
    "\t\t\t \n",
    "\t\t\trmse = np.sqrt(sse/self.NumSamples*self.Top[2]) # Taking the accumulated error for the whole training set and converting to RMSE.\n",
    "\n",
    "\t\t\tif rmse < bestrmse: # if the rmse for this epoch is lower than that for any previous epoch, then save the weights and biases\n",
    "\t\t\t\tbestrmse = rmse # Updated our best recorded rmse\n",
    "\t\t\t\t# print(f\"{bestrmse=}, {epoch=}\")\n",
    "\t\t\t\tself.saveKnowledge() # record weights and biasses for this epoch\n",
    "\t\t\t\tbestrmse, best_r2, actual, predicted = self.TestNetwork(self.TrainData, self.NumSamples, 0.2) # Not we are testing against the training data, not the test data\n",
    "\t\t\t\t# code added as unsw version seemed to have a bug\n",
    "\t\t\t\tbestTrain = bestrmse\n",
    "\t\t\t\t# debug\n",
    "\t\t\t\tif epoch % 50 == 0:\n",
    "\t\t\t\t\tprint(f\"{bestrmse=}, {bestTrain=}, {best_r2}, {epoch=}\")\n",
    "\n",
    "\t\t\tEr = np.append(Er, rmse)\n",
    "\t\t\t\n",
    "\t\t\tepoch=epoch+1  \n",
    "\n",
    "\t\treturn (Er,bestrmse, bestTrain, best_r2, epoch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalisedata(data, inputsize, outsize): # normalise the data between [0,1]\n",
    "\ttraindt = data[:,np.array(range(0,inputsize))]\t\n",
    "\tdt = np.amax(traindt, axis=0)\n",
    "\ttds = abs(traindt/dt) \n",
    "\treturn np.concatenate(( tds[:,range(0,inputsize)], data[:,range(inputsize,inputsize+outsize)]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import, Split and Normalize Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'AAPL'\n",
    "#dataset = 'Mackey'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "X.shape=(1718, 4), y.shape=(1718,)\n"
     ]
    }
   ],
   "source": [
    "if dataset == 'AAPL':\n",
    "\n",
    "    import yfinance as yf\n",
    "\n",
    "    # downside historical prices for AAPL\n",
    "    data = yf.download(\"AAPL\", start=\"2017-01-01\", end=\"2025-04-30\")\n",
    "    data = data['Adj Close']\n",
    "    \n",
    "    # Need to convert it to time series data\n",
    "    # Make each row our days of data\n",
    "    # The last column is the target\n",
    "    X = data.values\n",
    "    # convert X to be a matrix of 5 columns (5 days of data)\n",
    "    X = np.array([X[i:i+5] for i in range(len(X)-5)])\n",
    "    # The last column is the target\n",
    "    X, y = X[:,:-1], X[:,-1]\n",
    "\n",
    "    print(f\"{X.shape=}, {y.shape=}\")\n",
    "\n",
    "    # Split the data into train and test using sklearn\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y)\n",
    "    x_train, y_train\n",
    "\n",
    "    # Normalize the data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    # Combine x_train and y_train and x_test and y_test\n",
    "    # Doing this as its how the from scracth model was written\n",
    "    # X and y are only split out during the BP_GD function and done one row at a time\n",
    "    train_data = np.concatenate((x_train, y_train.reshape(-1,1)), axis=1)\n",
    "    test_data = np.concatenate((x_test, y_test.reshape(-1,1)), axis=1)\n",
    "\n",
    "    Hidden = 6\n",
    "    Input = 4\n",
    "    MaxTime = 1000\n",
    "\n",
    "    X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'Mackey':    \n",
    "    train_data = np.loadtxt(f\"{fpath}raw_data\\\\mackey_train.txt\") \n",
    "    test_data    = np.loadtxt(f\"{fpath}raw_data\\\\mackey_test.txt\") \n",
    "    Hidden = 5\n",
    "    Input = 4\n",
    "    MaxTime = 1000 \n",
    "\n",
    "    train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  is experimental run\n",
      "self.NumSamples: 1288\n",
      "bestrmse=109.81135859625536, bestTrain=109.81135859625536, -3.2384707843296345, epoch=0\n",
      "RMSE print performance for each experimental run\n",
      "trainPerf=array([109.8113586]), train_r2=array([-3.23847078])\n",
      "[107.64029478], test_r2=array([-3.34779307])\n",
      " print Epocs and Time taken for each experimental run\n",
      "[1.]\n",
      "[0.0728035]\n",
      " print mean and std of training performance\n",
      "np.mean(trainPerf)=109.81135859625536, np.mean(train_r2)=-3.2384707843296345, np.std(trainPerf)=0.0\n",
      "np.mean(testRMSE)=107.64029477719063, np.mean(test_r2)=-3.347793067129941\n",
      " print mean and std of computational time taken\n",
      "0.07280349731445312 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main(): \n",
    "\t\t\t\t\t\n",
    "\tOutput = 1 # We always have output of 1 for regression problems\n",
    "\tTrSamples =  train_data.shape[0]\n",
    "\tTestSize = test_data.shape[0]\n",
    "\n",
    "\tlearnRate = 0.1  \n",
    "\t#MaxTime = 100\n",
    "\n",
    "\tTopo = [Input, Hidden, Output] \n",
    "\tMaxRun = 1 # number of experimental runs \n",
    "\t \n",
    "\tMinCriteria = 95 #stop when learn 95 percent\n",
    "\n",
    "\ttrainTolerance = 0.2 # [eg 0.15 would be seen as 0] [ 0.81 would be seen as 1]\n",
    "\ttestTolerance = 0.49\n",
    " \n",
    "\ttrainPerf = np.zeros(MaxRun)\n",
    "\ttestPerf =  np.zeros(MaxRun)\n",
    "\n",
    "\ttrainRMSE =  np.zeros(MaxRun)\n",
    "\ttestRMSE =  np.zeros(MaxRun)\n",
    "\n",
    "\ttrain_r2 =  np.zeros(MaxRun)\n",
    "\ttest_r2 =  np.zeros(MaxRun)\n",
    "\n",
    "\tEpochs =  np.zeros(MaxRun)\n",
    "\tTime =  np.zeros(MaxRun)\n",
    "\n",
    "\tfor run in range(0, MaxRun  ):\n",
    "\t\tprint(run, ' is experimental run') \n",
    "\n",
    "\t\tfnn = Network(Topo, train_data, test_data, MaxTime, TrSamples, MinCriteria, learnRate) # The data is both X and Y combined, its split during the BP_GD function\n",
    "\t\tstart_time=time.time()\n",
    "\t\t(erEp,  trainRMSE[run] , trainPerf[run] , train_r2[run], Epochs[run]) = fnn.BP_GD()   \n",
    "\t\t#print(f\"{trainRMSE[run]=}, {trainPerf[run]=}, {Epochs[run]=}\")\n",
    "\n",
    "\t\tTime[run]  =time.time()-start_time\n",
    "\t\ttestRMSE[run], test_r2[run], actual, predicted = fnn.TestNetwork(test_data, TestSize, testTolerance)\n",
    "\n",
    "\tprint('RMSE print performance for each experimental run') \n",
    "\tprint(f\"{trainPerf=}, {train_r2=}\")\n",
    "\tprint(f\"{testRMSE}, {test_r2=}\")\n",
    "\tprint(' print Epocs and Time taken for each experimental run') \n",
    "\tprint(Epochs)\n",
    "\tprint(Time)\n",
    "\tprint(' print mean and std of training performance') \n",
    "\n",
    "\tprint(f\"{np.mean(trainPerf)=}, {np.mean(train_r2)=}, {np.std(trainPerf)=}\")\n",
    "\tprint(f\"{np.mean(testRMSE)=}, {np.mean(test_r2)=}\")\n",
    "\n",
    "\tprint(' print mean and std of computational time taken') \n",
    "\t\n",
    "\tprint(np.mean(Time), np.std(Time))\n",
    "\t\t\t\t \n",
    "\tplt.figure()\n",
    "\tplt.plot(erEp )\n",
    "\tplt.ylabel('error')  \n",
    "\tplt.savefig('out.png')\n",
    "\t\t\t \n",
    " \n",
    "if __name__ == \"__main__\": main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Dropbox\\\\Variance\\\\UNSW\\\\ZZSC5836\\\\MMM8_train.txt'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpath+\"raw_data\\\\MMM8_train.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(482, 5) (322, 5) (482, 5) (322, 5)\n",
      "[[0.175619 0.178449 0.18315  0.18243  0.1808  ]\n",
      " [0.548966 0.54996  0.548966 0.54907  0.535782]]\n",
      "[0.178785 0.180368 0.184445 0.18267  0.183006]\n"
     ]
    }
   ],
   "source": [
    "from numpy import *\n",
    "\n",
    "def read_data(run_num):\n",
    "    #Source - raw and processed data :  https://github.com/sydney-machine-learning/Bayesianneuralnet_stockmarket/tree/master/code/dataset\n",
    "    # five inputs (window size of 5) for 5 steps ahead (MMM dataset) https://finance.yahoo.com/quote/MMM/\n",
    "    #code to process raw data: https://github.com/sydney-machine-learning/Bayesianneuralnet_stockmarket/blob/master/code/data.py\n",
    "    data_in = genfromtxt(fpath+\"raw_data\\\\MMM8_train.txt\", delimiter=\" \")\n",
    "    data_inputx = data_in[:,0:5] # all features 0, 1, 2, 3, 4, 5, 6, 7 \n",
    "\n",
    "    #https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html#sklearn.preprocessing.Normalizer\n",
    "    #transformer = Normalizer().fit(data_inputx)  # fit does nothing.\n",
    "    #data_inputx = transformer.transform(data_inputx)\n",
    "    data_inputy = data_in[:,5:10] # this is target - so that last col is selected from data\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_inputx, data_inputy, test_size=0.40, random_state=run_num)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "x_train, x_test, y_train, y_test = read_data(1)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "print(x_train[:2])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
