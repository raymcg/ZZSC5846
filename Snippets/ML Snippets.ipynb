{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML Snippets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = [[1,2,3],[4,5,6],[7,8,9]]\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.23691392, 1.26598139, 1.81292007, 2.01415931, 1.87153602,\n",
       "        2.98562405],\n",
       "       [0.75445696, 0.95736585, 0.2428874 , 1.54887755, 2.31114727,\n",
       "        0.82717088],\n",
       "       [0.90697559, 0.58073798, 2.64198229, 2.02459886, 1.77226636,\n",
       "        0.41312762],\n",
       "       [2.82179916, 0.50143636, 2.56065491, 0.60651663, 1.71974766,\n",
       "        2.88321456],\n",
       "       [0.33660019, 1.62150803, 2.79162318, 1.276954  , 1.74943718,\n",
       "        1.245788  ],\n",
       "       [1.36761935, 2.86867982, 1.72544758, 1.08510106, 2.10606114,\n",
       "        0.67789981]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.random.rand(6,6)*3\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps and Order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import Data\n",
    "2. Clean / Reformat Data\n",
    "3. Define X and Y\n",
    "4. Normalize X\n",
    "5. Train/ Test/ CV Split\n",
    "6. Train Model, Run K-folds, Test Set Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data[:, :-1], data[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read text file via genfromtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import * # Needed for genfromtxt\n",
    "\n",
    "# data_in = genfromtxt(fpath+'abalone.csv', delimiter=\",\") # in case of csv data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 60/40 train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                    test_size=0.40, random_state=3)\n",
    "\n",
    "# The random state is a seed value for the random number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5) (3,)\n",
      "[[0.75445696 0.95736585 0.2428874  1.54887755 2.31114727]\n",
      " [1.23691392 1.26598139 1.81292007 2.01415931 1.87153602]\n",
      " [0.90697559 0.58073798 2.64198229 2.02459886 1.77226636]]\n",
      "[0.82717088 2.98562405 0.41312762]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test and CV Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train and Temp (Train and temp) : Train will be 60% of the entire dataset\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# CV and Test : CV and Test will both be 50% of the temp, which is 20% and 20% of the entire dataset\n",
    "X_cv, X_test, y_cv, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Now you have X_train, X_cv, X_test and corresponding y_train, y_cv, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Shuffle Split\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading Olivetti faces from https://ndownloader.figshare.com/files/5976027 to /opt/sklearn_data\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "olivetti = fetch_olivetti_faces(data_home='/opt/sklearn_data')\n",
    "\n",
    "strat_split = StratifiedShuffleSplit(n_splits=1, test_size=40, random_state=42)\n",
    "train_valid_idx, test_idx = next(strat_split.split(olivetti.data, olivetti.target))\n",
    "X_train_valid = olivetti.data[train_valid_idx]\n",
    "y_train_valid = olivetti.target[train_valid_idx]\n",
    "X_test = olivetti.data[test_idx]\n",
    "y_test = olivetti.target[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize / Scale Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.132e-04 -4.164e-02 -1.720e-01  1.165e+00 -2.318e-01]\n",
      " [-6.166e-01 -4.275e-01 -1.976e+00  2.433e-01  1.800e+00]\n",
      " [-4.218e-01 -8.985e-01  7.808e-01  1.186e+00 -6.906e-01]\n",
      " [ 2.023e+00 -9.976e-01  6.873e-01 -1.623e+00 -9.332e-01]\n",
      " [-1.150e+00  4.029e-01  9.528e-01 -2.953e-01 -7.960e-01]\n",
      " [ 1.663e-01  1.962e+00 -2.725e-01 -6.753e-01  8.520e-01]]\n"
     ]
    }
   ],
   "source": [
    "##### Normalize Input Data #######\n",
    "# Normalize data using min max scaler\n",
    "## We only normalize X, not X and y\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "### OR #### \n",
    "# Normalize data using standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform data \n",
    "#rescaledX = scaler.fit_transform(X) # this applies the scaler to the data and gives the mormalized data\n",
    "scaler.fit(X) # This first out scale to the data (where can then use it later to apply it to other data such as the test data)\n",
    "rescaledX = scaler.transform(X) # this applies the scaler to the data and gives the mormalized data\n",
    "\n",
    "np.set_printoptions(precision = 3) #Setting precision for the output\n",
    "print(rescaledX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting Normalization for Test Data / New Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It IS important that we normalize our X_test and X values that we may subsequently want to use for prediction.\n",
    "Because or weights depend on the scaling/ normalization used for training, we need to ensure it is consistent.\n",
    "This means we should use the same values used for normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized New Data Point: [[ 5.135  7.371  2.447 23.159 25.687]]\n"
     ]
    }
   ],
   "source": [
    "# Create a StandardScaler instance and fit it to the training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train) # Note that this is only fitting the scaler to the training data, it is not scaling the training data\n",
    "# Tranform is the process that scales the data\n",
    "\n",
    "# New data point\n",
    "new_data_point = np.array([[2,3,4,7.0, 8.0]])  # Example new data\n",
    "\n",
    "# Normalize the new data point using the same scaler\n",
    "normalized_new_data_point = scaler.transform(new_data_point) # Here we are transforming our data using the scaler that we fit to the training data\n",
    "\n",
    "print(\"Normalized New Data Point:\", normalized_new_data_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this method, all the values that are above the threshold are transformed into 1 and those equal to or below the threshold are transformed into 0. This method is useful when we deal with probabilities and need to convert the data into crisp values. \n",
    "\n",
    "Binarization is a common operation on text count data where the analyst can decide to only consider the presence or absence of a feature rather than a quantified number of occurrences for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3 -2 -1  0  1  2  3]]\n"
     ]
    }
   ],
   "source": [
    "sample_vector = np.array([-3,-2,-1,0,1,2,3]) # Create sample vector \n",
    "# reshape sample_vector into a 1x7 matrix\n",
    "sample_vector = sample_vector.reshape(1,-1)\n",
    "print(sample_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple NP Method \n",
    "threshold = 0  # Define your threshold here\n",
    "binary_y_train = (y_train > threshold).astype(int)\n",
    "binary_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector before binarization: [[-3 -2 -1  0  1  2  3]]\n",
      "Vector after binarization: [[0 0 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Binarizer.html\n",
    "# NOTE: Input needs to be a matrix, not a vector, so you should reshape any vectors with X.reshape(1, -1)\n",
    "\n",
    "binarizer = Binarizer(threshold=0.0).fit(sample_vector) \n",
    "binaryX = binarizer.transform(sample_vector) \n",
    "\n",
    "print(f\"Vector before binarization: {sample_vector}\")\n",
    "print(f\"Vector after binarization: {binaryX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring your y values are binary given its a classification problem\n",
    "binarizer = Binarizer(threshold=0.0).fit(y_train.reshape(-1,1)) \n",
    "binary_y_train = binarizer.transform(y_train.reshape(-1,1)) # Need to reshape as binarizer only takes 2D arrays\n",
    "binary_y_test = binarizer.transform(y_test.reshape(-1,1)) \n",
    "\n",
    "#convert binary y back to 1D array\n",
    "binary_y_train = binary_y_train.ravel()\n",
    "binary_y_test = binary_y_test.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Vector of Numbers to Be A Matrix For Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1,  2,  3,  4],\n",
       "        [ 2,  3,  4,  5],\n",
       "        [ 3,  4,  5,  6],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 5,  6,  7,  8],\n",
       "        [ 6,  7,  8,  9],\n",
       "        [ 7,  8,  9, 10],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [ 9, 10, 11, 12],\n",
       "        [10, 11, 12, 13]]),\n",
       " array([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15])\n",
    "\n",
    "# convert X to be a matrix of 5 columns (5 days of data)\n",
    "X = np.array([data[i:i+5] for i in range(len(data)-5)])\n",
    "# make X equal to the first 4 columns of X and y equal to the last column of X\n",
    "X, y = X[:,:-1], X[:,-1]\n",
    "X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv_results: {'fit_time': array([0.00033951, 0.        , 0.00099778, 0.00099778, 0.        ]), 'score_time': array([0., 0., 0., 0., 0.]), 'test_score': array([0.29828675, 0.2241492 , 0.15480127, 0.25519733, 0.17108715])}\n",
      "\n",
      "scores['test_neg_mean_squared_error']=array([-2807.16799618, -4890.37813089, -3360.65006947, -4663.03492141,\n",
      "       -5152.31964346])\n",
      "scores['train_r2']=array([0.32756806, 0.31459442, 0.33970628, 0.32065301, 0.27361929])\n",
      "\n",
      "cv_results: {'fit_time': array([0.00033951, 0.        , 0.00099778, 0.00099778, 0.        ]), 'score_time': array([0., 0., 0., 0., 0.]), 'test_score': array([0.29828675, 0.2241492 , 0.15480127, 0.25519733, 0.17108715])}\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    " \n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn import metrics  \n",
    "\n",
    "\n",
    "#Source: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html\n",
    "#Simulate splitting a dataset  into 5 folds\n",
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data[:150]\n",
    "\n",
    "y = diabetes.target[:150]\n",
    "lasso = linear_model.Lasso()\n",
    "\n",
    "#############################################\n",
    "#Single metric evaluation using cross_validate\n",
    "cv_results = cross_validate(lasso, X, y, cv=5)\n",
    "print(f\"cv_results: {cv_results}\\n\")\n",
    "\n",
    "scores = cross_validate(lasso, X, y, cv=5, scoring=('r2', 'neg_mean_squared_error'), return_train_score=True)\n",
    "print(f\"{scores['test_neg_mean_squared_error']=}\") \n",
    "print(f\"{scores['train_r2']=}\\n\")\n",
    "print(f\"cv_results: {cv_results}\")\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold cv for tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "Score for fold 1: loss of -697.5126953125; accuracy of 0.0%\n",
      "Training on fold 2...\n",
      "Score for fold 2: loss of -433.4169616699219; accuracy of 0.0%\n",
      "Training on fold 3...\n",
      "Score for fold 3: loss of -350.8194580078125; accuracy of 0.0%\n",
      "Training on fold 4...\n",
      "Score for fold 4: loss of -359.3729553222656; accuracy of 0.0%\n",
      "Training on fold 5...\n",
      "Score for fold 5: loss of -683.8095703125; accuracy of 0.0%\n",
      "Test set evaluation - Loss: -870.7814331054688, Accuracy: 0.0%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# First, split your data into a training and a hold-out test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to create model, required for KFold\n",
    "def create_model(input_dim):\n",
    "    # Define the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Prepare cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "##################################################\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for train, val in kfold.split(X_train, y_train):\n",
    "    # train is an array of indices of the datapoints to train on\n",
    "    # val is an array of indices of the datapoints for cross validation \n",
    "#################################################\n",
    "    model = create_model(input_dim=X_train.shape[1])\n",
    "    print(f'Training on fold {fold_no}...')\n",
    "\n",
    "    # Fit the model\n",
    "    history = model.fit(X_train[train], y_train[train], # selecting those index values from X_train and y_train to use as TRAINING data\n",
    "                        validation_data=(X_train[val], y_train[val]), # selecting those index values from X_train and y_train to use as VALIDATION data\n",
    "                        epochs=100,\n",
    "                        verbose=0)\n",
    "    \n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(X_train[val], y_train[val], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    fold_no += 1\n",
    "\n",
    "# Finally, after choosing and training your final model, evaluate it on the test set\n",
    "final_model = create_model(input_dim=X_train.shape[1])\n",
    "final_model.fit(X_train, y_train, epochs=100, verbose=0)\n",
    "test_scores = final_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test set evaluation - Loss: {test_scores[0]}, Accuracy: {test_scores[1]*100}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `kfold` is an instance of `KFold` from `sklearn.model_selection`, which is configured to split the data into a certain number of folds.\n",
    "- `kfold.split(X_train, y_train)` returns an iterator that generates pairs of indices. These pairs are the indices for the training set and the validation set for each fold.\n",
    "- Each `train` and `val` in the `for` loop are arrays of indices. `train` contains the indices of `X_train` and `y_train` that are going into making up the training set for the current fold, while `val` contains the indices of `X_train` and `y_train` that make up the validation set for the current fold.\n",
    "\n",
    "When you use `X_train[train]` and `y_train[train]`, you are indexing `X_train` and `y_train` with the `train` indices to extract the data points that will be used for training the model in the current fold.\n",
    "\n",
    "Similarly, `X_train[val]` and `y_train[val]` index `X_train` and `y_train` with the `val` indices to extract the data points that will be used for validating the model in the current fold.\n",
    "\n",
    "The `model.fit()` function then takes these indexed portions of the data to train the model and validate its performance.\n",
    "\n",
    "So, in summary:\n",
    "- `train` is an array of indices for the training data in the current fold.\n",
    "- `val` is an array of indices for the validation data in the current fold.\n",
    "- `X_train[train]` and `y_train[train]` are the subsets of the data used for training.\n",
    "- `X_train[val]` and `y_train[val]` are the subsets of the data used for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy and Results Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [1.1, 2, 1, 4]\n",
    "y_pred = [1.1, 1.8, 1.2, 3.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.02\n",
      "Root Mean Squared Error: 0.15\n",
      "R-squared (R2): 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Performance metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "# RMSE calculation\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', rmse)\n",
    "\n",
    "\n",
    "print(f\"R-squared (R2): {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn library for classification accuracy metric\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [0,0,1,1]\n",
    "y_pred = [0,0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[2 0]\n",
      " [1 1]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Negative</th>\n",
       "      <th>Predicted Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Negative</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Positive</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Predicted Negative  Predicted Positive\n",
       "Actual Negative                   2                   0\n",
       "Actual Positive                   1                   1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80         2\n",
      "           1       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.75         4\n",
      "   macro avg       0.83      0.75      0.73         4\n",
      "weighted avg       0.83      0.75      0.73         4\n",
      "\n",
      "Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Performance metrics\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Load the confusion matrix into a dataframe for better visualization\n",
    "import pandas as pd\n",
    "confusion_matrix_df = pd.DataFrame(confusion_matrix(y_test, y_pred),\n",
    "                                   index = ['Actual Negative','Actual Positive'],\n",
    "                                   columns = ['Predicted Negative','Predicted Positive'])\n",
    "display(confusion_matrix_df)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# if we want to access the indiv accuracy scores : see below for more details\n",
    "report_dict = classification_report(y_test, y_pred, output_dict =True) \n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "## for our ROC curve we need to generate the y_pred probabilities from our model\n",
    "# from sklearn.metrics import roc_curve, auc\n",
    "# Detailed example below\n",
    "# y_pred_proba = model.predict_proba(X_test)[:,1] # This gives us the probabilities for the positive class only\n",
    "# roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "#print(f\"ROC-AUC: {roc_auc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Precision: This measures the accuracy of the positive predictions for each class. For example, a precision of 0.83 for class 1.0 means that 83% of the instances classified as class 1.0 are actually class 1.0.\n",
    "\n",
    "    Recall: Also known as sensitivity, this measures the ability of the model to find all relevant instances in each class. For class 1.0, a recall of 0.72 indicates that the model correctly identified 72% of all actual class 1.0 instances.\n",
    "\n",
    "    F1-Score: This is the harmonic mean of precision and recall, providing a balance between the two. An F1-score of 0.77 for class 1.0 suggests a good balance between precision and recall for this class.\n",
    "\n",
    "    Support: This represents the number of true instances for each class in the dataset. For instance, there are 365 instances of class 1.0.\n",
    "\n",
    "    Overall Analysis:\n",
    "        The model performs best in classifying class 1.0, with the highest precision, recall, and F1-score.\n",
    "        Class 2.0 has a decent recall but lower precision, indicating it's better at identifying all relevant instances but at the cost of including more false positives.\n",
    "        Class 3.0 shows moderate performance in both precision and recall, suggesting a balanced but not highly accurate classification.\n",
    "        Class 4.0 has the poorest performance, with low precision and especially low recall, indicating both a high rate of false positives and a significant number of missed true class 4.0 instances.\n",
    "        The overall accuracy of the model is 0.66, meaning it correctly classifies 66% of the instances, regardless of class.\n",
    "\n",
    "    Macro vs. Weighted Average:\n",
    "        The macro average computes the metric independently for each class and then takes the average, treating all classes equally. The macro avg for precision, recall, and F1-score are relatively lower, indicating discrepancies in model performance across classes.\n",
    "        The weighted average takes the support into account, giving more weight to classes with more instances. The weighted averages are higher, suggesting that the model performs better on classes with more instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0]\n",
      " [1 1]]\n",
      "True Negatives (TN): 2\n",
      "False Positives (FP): 0\n",
      "False Negatives (FN): 1\n",
      "True Positives (TP): 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 1</th>\n",
       "      <th>Predicted 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual 1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted 1  Predicted 2\n",
       "Actual 1            2            0\n",
       "Actual 2            1            1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_test = [0,0,1,1]\n",
    "y_pred = [0,0,0,1]\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrix)\n",
    "\n",
    "# Extract TN, FP, FN, TP\n",
    "TN, FP, FN, TP = conf_matrix.ravel()\n",
    "# The order of the output from sklearn is usually TN, FP, FN, TP\n",
    "\n",
    "# Print the results\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "\n",
    "# Create a DataFrame from the confusion matrix\n",
    "cm_df = pd.DataFrame(conf_matrix, columns=['Predicted 1', 'Predicted 2'])\n",
    "cm_df.index = ['Actual 1', 'Actual 2']\n",
    "\n",
    "# Display the DataFrame\n",
    "display(cm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80         2\n",
      "           1       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.75         4\n",
      "   macro avg       0.83      0.75      0.73         4\n",
      "weighted avg       0.83      0.75      0.73         4\n",
      "\n",
      "\n",
      "Recall for class 0: 1.0\n",
      "Recall for class 1: 0.5\n",
      "Accuracy: 0.75\n",
      "F1-Score for class 0: 0.8\n",
      "F1-Score for class 1: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "y_test = [0, 0, 1, 1]  # True labels\n",
    "y_pred = [0, 0, 0, 1]  # Predicted labels\n",
    "\n",
    "# Generate the classification report : setting output_dict to True will return a dictionary of the classification report\n",
    "report_dict = classification_report(y_test, y_pred, output_dict =True)\n",
    "\n",
    "# Extract recall values for each class\n",
    "recall_0 = report_dict['0']['recall']\n",
    "recall_1 = report_dict['1']['recall']\n",
    "\n",
    "# Extract accuracy and f1-score\n",
    "accuracy = report_dict['accuracy']\n",
    "f1_score_0 = report_dict['0']['f1-score']\n",
    "f1_score_1 = report_dict['1']['f1-score']\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print()\n",
    "# Print the results\n",
    "print(f\"Recall for class 0: {recall_0}\")\n",
    "print(f\"Recall for class 1: {recall_1}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1-Score for class 0: {f1_score_0}\")\n",
    "print(f\"F1-Score for class 1: {f1_score_1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC_AUC Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(5,), max_iter=1000, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(5,), max_iter=1000, random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(5,), max_iter=1000, random_state=1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a synthetic binary classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=100, n_features=4, n_classes=2, random_state=1)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Create and train the neural network model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "model = MLPClassifier(hidden_layer_sizes=(5,), max_iter=1000, random_state=1)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 1.00\n"
     ]
    }
   ],
   "source": [
    "## for our ROC curve we need to generate the y_pred probabilities from our model\n",
    "\n",
    "y_pred_proba = model.predict_proba(X_test)[:,1] # This gives us the probabilities for the positive class only\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"ROC-AUC: {roc_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Results Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Fit the Model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=4000, verbose=0)\n",
    "\n",
    "# Evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "\n",
    "history.history['accuracy'] ## history is the parameter storing the results of the model run: tf records results for both the train and validation data\n",
    "history.history['val_accuracy'] ## history is the parameter storing the results of the model run: : tf records results for both the train and validation data\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGULARIZATION - MISSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three different ways to define a model using regularizers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_l1_LR = LogisticRegression(C=0.5, penalty='l1', tol=0.01, solver='saga')\n",
    "clf_l2_LR = LogisticRegression(C=0.5, penalty='l2', tol=0.01, solver='saga')\n",
    "clf_en_LR = LogisticRegression(C=0.5, penalty='elasticnet', solver='saga',\n",
    "                                   l1_ratio=0.5, tol=0.01)\n",
    "\n",
    "# C is the inverse of the regularization strength, smaller values specify stronger regularization\n",
    "# tol is the tolerance for stopping criteria (when loss is low enought to stop)\n",
    "# saga is an extension of sgd and supports l1 and l2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 Regularization with Tensor Flow\n",
    "from keras.regularizers import l2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(500, input_dim=2, activation='relu', kernel_regularizer= l2(0.001))) #### This is the line we add the regularizer to\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELS - Basic Models Missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  6]\n",
      " [10  5]\n",
      " [ 2  4]\n",
      " [ 7  8]\n",
      " [ 8  4]\n",
      " [ 4  5]\n",
      " [ 1  3]\n",
      " [ 6  7]]\n",
      "Coefficients: [ 2.01601678 -0.063808  ]\n",
      "Mean squared error: 0.05147509975513291\n",
      "Root Mean Squared Error: 0.22688124593084574\n",
      "Coefficient of determination: 0.998570136117913\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data for multifactor linear regression\n",
    "# X - independent variables (e.g., hours studied, sleep hours)\n",
    "# y - dependent variable (e.g., test scores)\n",
    "# X has two columsn and thus two input factors/ features\n",
    "\n",
    "X = np.array([\n",
    "    [1, 3], [2, 4], [3, 2], [4, 5], [5, 6],\n",
    "    [6, 7], [7, 8], [8, 4], [9, 9], [10, 5]\n",
    "])\n",
    "y = np.array([2.3, 4.4, 6.1, 8.2, 10.1, 12.3, 14.0, 16.2, 18.1, 20.5])\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "print(X_train)\n",
    "\n",
    "# Creating the model and fitting it to the training data\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions using the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients:', model.coef_)\n",
    "# The mean squared error\n",
    "print('Mean squared error:', mean_squared_error(y_test, y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "# RMSE calculation\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', rmse)\n",
    "\n",
    "print('Coefficient of determination:', r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Regressor: Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [2.00709605 0.01909926]\n",
      "Mean squared error: 0.02465293506369395\n",
      "Coefficient of determination: 0.9993151962482307\n",
      "Root Mean Squared Error: 0.15701253154985417\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Creating the model using SGDRegressor\n",
    "sgd_model = SGDRegressor(max_iter=1000, tol=1e-3, random_state=0)\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html\n",
    "sgd_model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions using the testing set\n",
    "y_pred_sgd = sgd_model.predict(X_test)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients:', sgd_model.coef_)\n",
    "# The mean squared error\n",
    "print('Mean squared error:', mean_squared_error(y_test, y_pred_sgd))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination:', r2_score(y_test, y_pred_sgd))\n",
    "\n",
    "# RMSE calculation\n",
    "rmse_sgd = np.sqrt(mean_squared_error(y_test, y_pred_sgd))\n",
    "print('Root Mean Squared Error:', rmse_sgd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression : Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy=0.956140350877193\n",
      "               Predicted Negative  Predicted Positive\n",
      "True Negative                  39                   4\n",
      "True Positive                   1                  70\n",
      "precision=0.9459459459459459, recall=0.9859154929577465\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Splitting data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the classifier\n",
    "clf = LogisticRegression(max_iter=10000)\n",
    "\n",
    "\n",
    "# Here is the only change you need for using a NN instead of logistic regression\n",
    "#from sklearn.neural_network import MLPClassifier\n",
    "#clf = MLPClassifier(alpha=1e-05, hidden_layer_sizes=(5,), random_state=1, solver='lbfgs')\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculating accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Creating the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Loading the confusion matrix into a DataFrame\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, index=['True Negative', 'True Positive'], \n",
    "                              columns=['Predicted Negative', 'Predicted Positive'])\n",
    "\n",
    "print(f\"{accuracy=}\")\n",
    "print(conf_matrix_df)\n",
    "\n",
    "# Calculating precision and recall\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(f\"{precision=}, {recall=}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression : Multi Class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 0</th>\n",
       "      <th>Predicted 1</th>\n",
       "      <th>Predicted 2</th>\n",
       "      <th>Predicted 3</th>\n",
       "      <th>Predicted 4</th>\n",
       "      <th>Predicted 5</th>\n",
       "      <th>Predicted 6</th>\n",
       "      <th>Predicted 7</th>\n",
       "      <th>Predicted 8</th>\n",
       "      <th>Predicted 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True 0</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True 1</th>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True 2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True 3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True 4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True 5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True 6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True 7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True 8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True 9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Predicted 0  Predicted 1  Predicted 2  Predicted 3  Predicted 4  \\\n",
       "True 0           33            0            0            0            0   \n",
       "True 1            0           28            0            0            0   \n",
       "True 2            0            0           33            0            0   \n",
       "True 3            0            0            0           33            0   \n",
       "True 4            0            1            0            0           45   \n",
       "True 5            0            0            1            0            0   \n",
       "True 6            0            0            0            0            0   \n",
       "True 7            0            0            0            0            0   \n",
       "True 8            0            0            0            0            0   \n",
       "True 9            0            0            0            1            0   \n",
       "\n",
       "        Predicted 5  Predicted 6  Predicted 7  Predicted 8  Predicted 9  \n",
       "True 0            0            0            0            0            0  \n",
       "True 1            0            0            0            0            0  \n",
       "True 2            0            0            0            0            0  \n",
       "True 3            1            0            0            0            0  \n",
       "True 4            0            0            0            0            0  \n",
       "True 5           44            1            0            0            1  \n",
       "True 6            1           34            0            0            0  \n",
       "True 7            1            0           33            0            0  \n",
       "True 8            1            0            0           29            0  \n",
       "True 9            0            0            0            1           38  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.9722222222222222,\n",
       " 0.9735814591088425,\n",
       " 0.9743702791014647,\n",
       " array([[33,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0, 28,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0, 33,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0, 33,  0,  1,  0,  0,  0,  0],\n",
       "        [ 0,  1,  0,  0, 45,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  1,  0,  0, 44,  1,  0,  0,  1],\n",
       "        [ 0,  0,  0,  0,  0,  1, 34,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  1,  0, 33,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  1,  0,  0, 29,  0],\n",
       "        [ 0,  0,  0,  1,  0,  0,  0,  0,  1, 38]], dtype=int64))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits  # Example multi-class dataset\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load a multi-class dataset\n",
    "data = load_digits()  # This is a dataset with 10 classes (digits 0-9)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Splitting data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the classifier\n",
    "classifier = LogisticRegression(max_iter=10000, multi_class='multinomial')\n",
    "# For multi_class = 'multinomial' the probability distribution over the various classes is modeled using a softmax function.\n",
    "# Multinomial logistic regression predicts the probabilities of each class and the class with the highest probability is the final prediction.\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Calculating accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Creating the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "# Load the confusion matrix into a DataFrame for better visualization\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, index=['True 0', 'True 1', 'True 2', 'True 3', 'True 4', 'True 5', 'True 6', 'True 7', 'True 8', 'True 9'], \n",
    "                              columns=['Predicted 0', 'Predicted 1', 'Predicted 2', 'Predicted 3', 'Predicted 4', 'Predicted 5', 'Predicted 6', 'Predicted 7', 'Predicted 8', 'Predicted 9'])\n",
    "display(conf_matrix_df)\n",
    "\n",
    "# For multi-class, precision and recall can be calculated for each class and averaged\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Output\n",
    "accuracy, precision, recall, conf_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing a Large Number of Different Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors : 0.97\n",
      "Linear SVM : 0.88\n",
      "RBF SVM : 0.97\n",
      "Gaussian Process : 0.97\n",
      "Decision Tree : 0.95\n",
      "Random Forest : 0.95\n",
      "Neural Net : 0.90\n",
      "AdaBoost : 0.93\n",
      "Naive Bayes : 0.88\n",
      "QDA : 0.85\n",
      "Nearest Neighbors : 0.93\n",
      "Linear SVM : 0.40\n",
      "RBF SVM : 0.88\n",
      "Gaussian Process : 0.90\n",
      "Decision Tree : 0.78\n",
      "Random Forest : 0.75\n",
      "Neural Net : 0.88\n",
      "AdaBoost : 0.82\n",
      "Naive Bayes : 0.70\n",
      "QDA : 0.72\n",
      "Nearest Neighbors : 0.95\n",
      "Linear SVM : 0.93\n",
      "RBF SVM : 0.95\n",
      "Gaussian Process : 0.93\n",
      "Decision Tree : 0.95\n",
      "Random Forest : 0.95\n",
      "Neural Net : 0.95\n",
      "AdaBoost : 0.95\n",
      "Naive Bayes : 0.95\n",
      "QDA : 0.93\n"
     ]
    }
   ],
   "source": [
    "# Taken from sklearn website\n",
    "https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.datasets import make_circles, make_classification, make_moons\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "names = [\n",
    "    \"Nearest Neighbors\",\n",
    "    \"Linear SVM\",\n",
    "    \"RBF SVM\",\n",
    "    \"Gaussian Process\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"Neural Net\",\n",
    "    \"AdaBoost\",\n",
    "    \"Naive Bayes\",\n",
    "    \"QDA\",\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025, random_state=42),\n",
    "    SVC(gamma=2, C=1, random_state=42),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0), random_state=42),\n",
    "    DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "    RandomForestClassifier(\n",
    "        max_depth=5, n_estimators=10, max_features=1, random_state=42\n",
    "    ),\n",
    "    MLPClassifier(alpha=1, max_iter=1000, random_state=42),\n",
    "    AdaBoostClassifier(random_state=42),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "]\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1\n",
    ")\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [\n",
    "    make_moons(noise=0.3, random_state=0),\n",
    "    make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "    linearly_separable,\n",
    "]\n",
    "\n",
    "i = 1\n",
    "# iterate over datasets\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X, y = ds\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.4, random_state=42\n",
    "    )\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "       \n",
    "        clf = make_pipeline(StandardScaler(), clf)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        print(f\"{name} : {score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "\n",
    "# import mlp regression model\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solver = 'sgd' # Stochastic Gradient Descent\n",
    "solver = 'adam' # Adam\n",
    "activation = 'relu' # Rectified Linear Unit (ReLU)\n",
    "# activation = 'logistic' # Logistic (sigmoid)\n",
    "alpha = 0.0001 # L2 penalty (regularization term) parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For solver chocies, bvoth adam and sgd can be used for both regression and classification.\n",
    "\n",
    "Stochastic Gradient Descent (SGD): Good for large datasets, but requires careful tuning of the learning rate and can benefit from momentum.\n",
    "\n",
    "Adam: Combines advantages of AdaGrad and RMSProp and is generally robust for a wide range of problems. It adjusts the learning rate dynamically and is efficient for large datasets and high-dimensional spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation used (relu, sigmoid, linear etc) for sklearn models are used ONLY for the HIDDEN layers. \n",
    "The model will select an appropriate action method depending on whether we've chose the mlplcassifier or mplregressor model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKLearn Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape=(70, 4), X_test.shape=(30, 4)\n",
      "Training set predictions:\n",
      " [1 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 0 0 1 1 1 0 0 1 1\n",
      " 0 0 0 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 0 0 1 0]\n",
      "Test set predictions:\n",
      " [0 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 0 1 1 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1 Hidden layer\n",
    "nn1 = MLPClassifier(hidden_layer_sizes=(8,), random_state=2, max_iter=100, activation = activation, solver=solver,  \n",
    "                alpha = alpha, learning_rate_init=0.01 )\n",
    "        #https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "\n",
    "# 2 Hidden layers\n",
    "nn2 = MLPClassifier(hidden_layer_sizes=(8,4), random_state=2, max_iter=100,activation = activation, solver=solver,\n",
    "                alpha = alpha, learning_rate='constant', learning_rate_init=0.01)\n",
    "        #hidden_layer_sizes=(hidden,hidden, hidden) would implement 3 hidden layers\n",
    " \n",
    "# Train the model using the training sets\n",
    "nn1.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred_test = nn1.predict(X_test)\n",
    "y_pred_train = nn1.predict(X_train)\n",
    "\n",
    "print(f\"{X_train.shape=}, {X_test.shape=}\")\n",
    "print(f\"Training set predictions:\\n {y_pred_train}\")\n",
    "print(f\"Test set predictions:\\n {y_pred_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKLearn Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Hidden layer\n",
    "nn1 = MLPRegressor(hidden_layer_sizes=(8,), random_state=2, max_iter=1000, activation = 'relu', solver='adam',  \n",
    "                alpha = 0.1, learning_rate_init=0.01, tol=0.01 )\n",
    "        #https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html\n",
    "\n",
    "# Train the model using the training sets\n",
    "nn1.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred_test = nn1.predict(X_test)\n",
    "y_pred_train = nn1.predict(X_train)\n",
    "\n",
    "print(f\"{X_train.shape=}, {X_test.shape=}\")\n",
    "print(f\"Training set predictions:\\n {y_pred_train}\")\n",
    "print(f\"Test set predictions:\\n {y_pred_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 25)                125       \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 26        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 151\n",
      "Trainable params: 151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.9624 - accuracy: 0.3857 - val_loss: 0.8870 - val_accuracy: 0.4000\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.9404 - accuracy: 0.3857 - val_loss: 0.8707 - val_accuracy: 0.4000\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.9214 - accuracy: 0.3857 - val_loss: 0.8550 - val_accuracy: 0.4000\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.9020 - accuracy: 0.3714 - val_loss: 0.8398 - val_accuracy: 0.4000\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.8831 - accuracy: 0.3714 - val_loss: 0.8249 - val_accuracy: 0.4000\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.8653 - accuracy: 0.3714 - val_loss: 0.8103 - val_accuracy: 0.4000\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.8477 - accuracy: 0.3714 - val_loss: 0.7961 - val_accuracy: 0.4000\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.8314 - accuracy: 0.3714 - val_loss: 0.7825 - val_accuracy: 0.4000\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.8145 - accuracy: 0.3714 - val_loss: 0.7694 - val_accuracy: 0.4000\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.7988 - accuracy: 0.3714 - val_loss: 0.7565 - val_accuracy: 0.4333\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.7837 - accuracy: 0.3571 - val_loss: 0.7440 - val_accuracy: 0.4000\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.7694 - accuracy: 0.3571 - val_loss: 0.7319 - val_accuracy: 0.4333\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.7551 - accuracy: 0.3571 - val_loss: 0.7202 - val_accuracy: 0.4667\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.7406 - accuracy: 0.4000 - val_loss: 0.7087 - val_accuracy: 0.4667\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.7277 - accuracy: 0.4000 - val_loss: 0.6978 - val_accuracy: 0.4667\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.7143 - accuracy: 0.4143 - val_loss: 0.6876 - val_accuracy: 0.4667\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.7019 - accuracy: 0.4286 - val_loss: 0.6777 - val_accuracy: 0.5000\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6900 - accuracy: 0.4429 - val_loss: 0.6679 - val_accuracy: 0.5333\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6788 - accuracy: 0.4571 - val_loss: 0.6580 - val_accuracy: 0.5333\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6669 - accuracy: 0.4857 - val_loss: 0.6482 - val_accuracy: 0.5333\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6550 - accuracy: 0.5000 - val_loss: 0.6385 - val_accuracy: 0.5333\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6440 - accuracy: 0.5429 - val_loss: 0.6289 - val_accuracy: 0.5333\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6330 - accuracy: 0.5286 - val_loss: 0.6194 - val_accuracy: 0.5000\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6216 - accuracy: 0.5000 - val_loss: 0.6103 - val_accuracy: 0.5000\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.6111 - accuracy: 0.5286 - val_loss: 0.6012 - val_accuracy: 0.5667\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6008 - accuracy: 0.5857 - val_loss: 0.5923 - val_accuracy: 0.6667\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5913 - accuracy: 0.6286 - val_loss: 0.5836 - val_accuracy: 0.7333\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5816 - accuracy: 0.6857 - val_loss: 0.5751 - val_accuracy: 0.8000\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5725 - accuracy: 0.7429 - val_loss: 0.5669 - val_accuracy: 0.9000\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5633 - accuracy: 0.8286 - val_loss: 0.5588 - val_accuracy: 0.9333\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5545 - accuracy: 0.8857 - val_loss: 0.5508 - val_accuracy: 0.9333\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5455 - accuracy: 0.9286 - val_loss: 0.5429 - val_accuracy: 0.9333\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5371 - accuracy: 0.9286 - val_loss: 0.5352 - val_accuracy: 0.9333\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5285 - accuracy: 0.9714 - val_loss: 0.5275 - val_accuracy: 0.9333\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5200 - accuracy: 0.9714 - val_loss: 0.5199 - val_accuracy: 0.9333\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5114 - accuracy: 0.9714 - val_loss: 0.5122 - val_accuracy: 0.9333\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5029 - accuracy: 0.9714 - val_loss: 0.5046 - val_accuracy: 0.9333\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4948 - accuracy: 0.9714 - val_loss: 0.4972 - val_accuracy: 0.9333\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4864 - accuracy: 0.9714 - val_loss: 0.4898 - val_accuracy: 0.9333\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4783 - accuracy: 0.9714 - val_loss: 0.4825 - val_accuracy: 0.9333\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4705 - accuracy: 0.9714 - val_loss: 0.4755 - val_accuracy: 0.9333\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4631 - accuracy: 0.9714 - val_loss: 0.4687 - val_accuracy: 0.9333\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4554 - accuracy: 0.9714 - val_loss: 0.4620 - val_accuracy: 0.9333\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4483 - accuracy: 0.9571 - val_loss: 0.4554 - val_accuracy: 0.9333\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4413 - accuracy: 0.9571 - val_loss: 0.4488 - val_accuracy: 0.9333\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4344 - accuracy: 0.9714 - val_loss: 0.4425 - val_accuracy: 0.9333\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4275 - accuracy: 0.9714 - val_loss: 0.4363 - val_accuracy: 0.9333\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4208 - accuracy: 0.9714 - val_loss: 0.4303 - val_accuracy: 0.9333\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4139 - accuracy: 0.9714 - val_loss: 0.4243 - val_accuracy: 0.9333\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4071 - accuracy: 0.9714 - val_loss: 0.4184 - val_accuracy: 0.9333\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4005 - accuracy: 0.9714 - val_loss: 0.4125 - val_accuracy: 0.9333\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3940 - accuracy: 0.9714 - val_loss: 0.4067 - val_accuracy: 0.9333\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3875 - accuracy: 0.9714 - val_loss: 0.4009 - val_accuracy: 0.9333\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3811 - accuracy: 0.9714 - val_loss: 0.3950 - val_accuracy: 0.9333\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3748 - accuracy: 0.9714 - val_loss: 0.3892 - val_accuracy: 0.9333\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3687 - accuracy: 0.9714 - val_loss: 0.3837 - val_accuracy: 0.9333\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3626 - accuracy: 0.9714 - val_loss: 0.3782 - val_accuracy: 0.9333\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3567 - accuracy: 0.9714 - val_loss: 0.3726 - val_accuracy: 0.9333\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3509 - accuracy: 0.9714 - val_loss: 0.3672 - val_accuracy: 0.9333\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3451 - accuracy: 0.9714 - val_loss: 0.3619 - val_accuracy: 0.9333\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3395 - accuracy: 0.9714 - val_loss: 0.3567 - val_accuracy: 0.9333\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3337 - accuracy: 0.9714 - val_loss: 0.3516 - val_accuracy: 0.9333\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3280 - accuracy: 0.9714 - val_loss: 0.3466 - val_accuracy: 0.9333\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3224 - accuracy: 0.9714 - val_loss: 0.3417 - val_accuracy: 0.9333\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3169 - accuracy: 0.9714 - val_loss: 0.3369 - val_accuracy: 0.9333\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3115 - accuracy: 0.9714 - val_loss: 0.3322 - val_accuracy: 0.9333\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3063 - accuracy: 0.9857 - val_loss: 0.3275 - val_accuracy: 0.9333\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3009 - accuracy: 0.9857 - val_loss: 0.3228 - val_accuracy: 0.9333\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2958 - accuracy: 0.9857 - val_loss: 0.3182 - val_accuracy: 0.9333\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.2906 - accuracy: 0.9857 - val_loss: 0.3137 - val_accuracy: 0.9333\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2857 - accuracy: 0.9857 - val_loss: 0.3092 - val_accuracy: 0.9333\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2810 - accuracy: 0.9857 - val_loss: 0.3049 - val_accuracy: 0.9333\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2761 - accuracy: 0.9857 - val_loss: 0.3008 - val_accuracy: 0.9333\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2716 - accuracy: 0.9857 - val_loss: 0.2966 - val_accuracy: 0.9333\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2669 - accuracy: 0.9857 - val_loss: 0.2925 - val_accuracy: 0.9333\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2623 - accuracy: 0.9857 - val_loss: 0.2886 - val_accuracy: 0.9333\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2579 - accuracy: 0.9857 - val_loss: 0.2849 - val_accuracy: 0.9333\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2535 - accuracy: 0.9857 - val_loss: 0.2814 - val_accuracy: 0.9333\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2492 - accuracy: 0.9857 - val_loss: 0.2779 - val_accuracy: 0.9333\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2452 - accuracy: 0.9857 - val_loss: 0.2743 - val_accuracy: 0.9333\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2411 - accuracy: 0.9857 - val_loss: 0.2708 - val_accuracy: 0.9333\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2371 - accuracy: 0.9857 - val_loss: 0.2673 - val_accuracy: 0.9333\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2332 - accuracy: 0.9857 - val_loss: 0.2639 - val_accuracy: 0.9333\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2296 - accuracy: 0.9857 - val_loss: 0.2606 - val_accuracy: 0.9333\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2259 - accuracy: 0.9857 - val_loss: 0.2573 - val_accuracy: 0.9333\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2222 - accuracy: 0.9857 - val_loss: 0.2539 - val_accuracy: 0.9333\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2185 - accuracy: 0.9857 - val_loss: 0.2505 - val_accuracy: 0.9333\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2148 - accuracy: 0.9857 - val_loss: 0.2472 - val_accuracy: 0.9333\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2112 - accuracy: 0.9857 - val_loss: 0.2440 - val_accuracy: 0.9333\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2077 - accuracy: 0.9857 - val_loss: 0.2409 - val_accuracy: 0.9333\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2043 - accuracy: 0.9857 - val_loss: 0.2379 - val_accuracy: 0.9333\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2010 - accuracy: 0.9857 - val_loss: 0.2349 - val_accuracy: 0.9333\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.1977 - accuracy: 0.9857 - val_loss: 0.2323 - val_accuracy: 0.9333\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1946 - accuracy: 0.9857 - val_loss: 0.2297 - val_accuracy: 0.9333\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1916 - accuracy: 0.9857 - val_loss: 0.2272 - val_accuracy: 0.9333\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1885 - accuracy: 0.9857 - val_loss: 0.2248 - val_accuracy: 0.9333\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1857 - accuracy: 0.9857 - val_loss: 0.2224 - val_accuracy: 0.9333\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1828 - accuracy: 0.9857 - val_loss: 0.2200 - val_accuracy: 0.9333\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1799 - accuracy: 0.9857 - val_loss: 0.2177 - val_accuracy: 0.9333\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1771 - accuracy: 0.9857 - val_loss: 0.2154 - val_accuracy: 0.9333\n",
      "\n",
      "Train Accuracy: 0.986, Test Accuracy: 0.933\n"
     ]
    }
   ],
   "source": [
    "def build_tf_model():\n",
    "    # Define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, input_dim=X_train.shape[1], activation='relu')) # Hidden layer 1: 25 neurons, shows that the input layer has X_train.shape[1] neurons\n",
    "    model.add(Dense(1, activation='sigmoid')) # Output layer: 1 neuron, sigmoid activation\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) # Build the model (build it, not run it)\n",
    "\n",
    "    model.summary() # Print model summary\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_tf_model()\n",
    "\n",
    "# Fit model : Runs the model and outputs the results into history\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "_, train_acc = model.evaluate(X_train, y_train, verbose=0) # The two method outputs are loss and accuracy (accuracy because we defined that as our metric)\n",
    "_, test_acc = model.evaluate(X_test, y_test, verbose=0) # The two method outputs are loss and accuracy (accuracy because we defined that as our metric)\n",
    "print('\\nTrain Accuracy: %.3f, Test Accuracy: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees : scikitlearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1.0\n",
      "Accuracy:  1.0\n",
      "Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Loading a sample dataset - Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Creating and training the Decision Tree Classifier\n",
    "tree_clf = DecisionTreeClassifier(max_features=\"sqrt\", random_state=25, min_samples_leaf=1, max_depth=20)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Score: {tree_clf.score(X_test, y_test)}\") # this is the accuracy score\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = tree_clf.predict(X_test)\n",
    "\n",
    "# Calculating the accuracy and other metrics\n",
    "accuracy = accuracy_score(y_test, y_pred) # a diiferent way to get to the accuracy score\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "\n",
    "print('Accuracy: ', accuracy)\n",
    "print('Report\\n', report)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse: 20.45, r2: 0.73\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "y = raw_df.values[1::2, 2]\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Creating and training the Decision Tree Regressor\n",
    "tree_reg = DecisionTreeRegressor(max_depth=4, random_state=42)\n",
    "tree_reg.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = tree_reg.predict(X_test)\n",
    "\n",
    "# Calculating mean squared error and R^2 score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"mse: {mse:.2f}, r2: {r2:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=0,\n",
    "                           random_state=42, n_clusters_per_class=1)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the AdaBoost classifier\n",
    "# Using default settings: decision tree stumps as weak learners\n",
    "ada_clf = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "\"\"\"\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    base_estimator=decision_tree_model, n_estimators=10,\n",
    "    algorithm=\"SAMME.R\", learning_rate= 1)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "ada_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = ada_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters\n",
    "\n",
    "1. **base_estimator**: This defines the type of weak learner to use. By default, AdaBoost uses decision tree stumps (i.e., `DecisionTreeClassifier(max_depth=1)`). However, you can specify any other model as the base estimator.\n",
    "\n",
    "2. **n_estimators**: This parameter sets the number of weak learners to be used in the boosting process. A higher number of estimators can improve the model's performance but also increases the risk of overfitting and computational cost.\n",
    "\n",
    "3. **learning_rate**: This parameter shrinks the contribution of each classifier. There is a trade-off between learning_rate and n_estimators. A smaller value of learning_rate requires a larger number of n_estimators for the same level of accuracy.\n",
    "\n",
    "4. **algorithm**: AdaBoost supports two algorithms: `SAMME` and `SAMME.R`. The `SAMME.R` algorithm typically converges faster than `SAMME`, needing fewer boosting iterations. `SAMME.R` uses the class probabilities whereas `SAMME` uses classifications.\n",
    "\n",
    "5. **random_state**: This controls the randomness of the algorithm. Providing a fixed random_state ensures reproducibility of the results.\n",
    "\n",
    "Adjusting these parameters can help tailor the AdaBoost model to your specific dataset and problem. It's often useful to use techniques like grid search or random search for hyperparameter tuning to find the most effective combination of these parameters. \n",
    "\n",
    "Keep in mind that while more complex base estimators and a higher number of estimators might improve model performance, they also increase the risk of overfitting and the computational cost. As with any machine learning model, there's a balance to be struck between complexity, performance, and computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.208861361528038, 0.9153342280466539)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the Boston housing dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "y = raw_df.values[1::2, 2]\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the GradientBoostingRegressor model\n",
    "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = gbr.predict(X_test)\n",
    "\n",
    "# Calculate and print performance metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "mse, r2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9666666666666667\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0  8  1]\n",
      " [ 0  0 11]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Setosa</th>\n",
       "      <th>Versicolor</th>\n",
       "      <th>Virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Setosa</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Versicolor</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Virginica</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Setosa  Versicolor  Virginica\n",
       "Setosa          10           0          0\n",
       "Versicolor       0           8          1\n",
       "Virginica        0           0         11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 1</th>\n",
       "      <th>Predicted 2</th>\n",
       "      <th>Predicted 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual 1</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 2</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted 1  Predicted 2  Predicted 3\n",
       "Actual 1           10            0            0\n",
       "Actual 2            0            8            1\n",
       "Actual 3            0            0           11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the GradientBoostingClassifier model\n",
    "gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = gbc.predict(X_test)\n",
    "\n",
    "# Calculate and print performance metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# load the confusion matrix into a dataframe for better visualization\n",
    "import pandas as pd\n",
    "confusion_matrix_df = pd.DataFrame(conf_matrix,\n",
    "                                   index = ['Setosa','Versicolor','Virginica'],\n",
    "                                   columns = ['Setosa','Versicolor','Virginica'])\n",
    "display(confusion_matrix_df)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, index=data.target_names, columns=data.target_names)\n",
    "cm_df = pd.DataFrame(conf_matrix, columns=['Predicted 1', 'Predicted 2', 'Predicted 3'])\n",
    "cm_df.index = ['Actual 1', 'Actual 2', 'Actual 3']\n",
    "\n",
    "# Display the DataFrame\n",
    "display(cm_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$S(x) =  1 / (1 + e^{-x})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before sigmoid activation: [-3 -2 -1  0  1  2  3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.047, 0.119, 0.269, 0.5  , 0.731, 0.881, 0.953])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_vector = np.array([-3,-2,-1,0,1,2,3]) # Create sample vector\n",
    "\n",
    "print(f\"Before sigmoid activation: {sample_vector}\") # Note: We don't apply sigmoid to y, we apply it to generate y_predicted from our z values\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# All our negative input values will have a transformed value below 0.5    \n",
    "# All our positive inpu values will have a transformed value above 0.5\n",
    "sigmoid(sample_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.002 0.002 0.003 ... 0.997 0.997 0.998]\n",
      "[0.002 0.002 0.003 ... 0.003 0.003 0.002]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    s=1/(1+np.exp(-x)) # This implements the sigmoid function\n",
    "    ds=s*(1-s)  # This gives the derivative \n",
    "    return s,ds\n",
    "    \n",
    "x=np.arange(-6,6,0.01)\n",
    "\n",
    "s, ds = sigmoid(x)\n",
    "print(s)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.718  7.389 20.086 54.598  2.718  7.389 20.086]\n",
      "114.98389973429897\n",
      "[0.024 0.064 0.175 0.475 0.024 0.064 0.175]\n"
     ]
    }
   ],
   "source": [
    "a = [1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0]\n",
    "print(np.exp(a))\n",
    "print(np.sum(np.exp(a)))\n",
    "\n",
    "# exp is the numpy function for the natural log e (so a to the power log e)\n",
    "soft_max = np.exp(a) / np.sum(np.exp(a)) \n",
    "print(soft_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "# model.add(Dropout(0.4)) # Dropout layer with a 40% dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITH DROPOUTS, JUST THE ONE LINE ADDED\n",
    "model = Sequential()\n",
    "model.add(Dense(500, input_dim=2, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# WITH DROPOUTS, JUST THE ONE IS DIFFERENT\n",
    "model = Sequential()\n",
    "model.add(Dense(500, input_dim=2, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of classification problems, you need a proper way to represent the class values or outcomes, especially in multi-class problems.\n",
    "\n",
    "A one-hot encoding is a representation of categorical variables as binary vectors. The categorical values are typically mapped to integer values and then each integer value is represented as a binary vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cold' 'cold' 'warm' 'cold' 'hot' 'hot' 'warm' 'cold' 'warm' 'hot']\n"
     ]
    }
   ],
   "source": [
    "#Source: https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\n",
    "\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Define example\n",
    "data_to_encode = ['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']\n",
    "values = array(data_to_encode)\n",
    "print(values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 2 0 1 1 2 0 2 1]\n"
     ]
    }
   ],
   "source": [
    "#### STEP 1 #### : CONVERT TO INTEGER VALUES\n",
    "\n",
    "# Here we are converting cold to be 0, hot to be 1, and warm to be 2\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Hot Encoded\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "### STEP 2 ### : ONE HOT ENCODE\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False) # sparse=False ensures we get a non-sparse matrix\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1) # Reshape to be a 2D array with 1 column\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print('One Hot Encoded')\n",
    "print(onehot_encoded)\n",
    "\n",
    "# Our first row shows the result for cold, which is 0, so we have [1,0,0]\n",
    "# Our second row shows the result for cold, which is 0, so we have [1,0,0]\n",
    "# Our third row shows the result for warm, which is 2, so we have [0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inverted\n",
      "['cold']\n"
     ]
    }
   ],
   "source": [
    "# Invert first example if we want to work backwards when we get our predicitons\n",
    "inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
    "print('\\nInverted')\n",
    "print(inverted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRIDSEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = genfromtxt(\"C:/Dropbox/Variance/UNSW/ZZSC5836/raw_data/pima.csv\", delimiter=\",\")\n",
    " \n",
    "data_inputx = data_in[:,0:8] # all features 0, 1, 2, 3, 4, 5, 6, 7 \n",
    "data_inputy = data_in[:,-1] # this is target - so that last col is selected from data\n",
    "\n",
    "# split to training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_inputx, data_inputy, test_size=0.40, random_state=23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters discovered:\n",
      " {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "\n",
      "Report results on the test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.81      0.76       197\n",
      "         1.0       0.56      0.42      0.48       111\n",
      "\n",
      "    accuracy                           0.67       308\n",
      "   macro avg       0.64      0.62      0.62       308\n",
      "weighted avg       0.66      0.67      0.66       308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model generation object  \n",
    "mlp = MLPClassifier(max_iter=1000)\n",
    "\n",
    "# Parameter Space\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,), (50,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "    }\n",
    "\n",
    "# Search through parameter space, build models for different parameters\n",
    "clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n",
    "# Grid Search for MLPClassifier as the model, parameter_space as the space of parameters, n_jobs=-1 to use all processors, cv=3 for 3-fold cross validation\n",
    "\n",
    "#clf.fit(X_train, binary_y_train)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# Best parameteR set\n",
    "print('Best parameters discovered:\\n', clf.best_params_)\n",
    "\n",
    "########## EVALUATION AND PREDICTION ##########\n",
    "y_true, y_pred = y_test , clf.predict(x_test)\n",
    "## Very interesting is the clf.predict selects the optimal parameters from the gridsearch cv results\n",
    "\n",
    "\n",
    "print('\\nReport results on the test set:')\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "\n",
    "#acc_test = accuracy_score(y_pred, y_true) \n",
    "\n",
    "y_pred_train = clf.predict(x_train)\n",
    "#acc_train = accuracy_score(y_pred_train, y_train) \n",
    "\n",
    "#cm = confusion_matrix(y_pred, y_true) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV:\n",
    "\n",
    "    Exhaustive Search: It performs an exhaustive search over a specified parameter space. This means it tries every combination of the provided hyperparameter values.\n",
    "    Time-Consuming: As it evaluates all possible combinations, it can be very time-consuming, especially for large datasets and complex models.\n",
    "    Precision: Since it checks all combinations, it’s more likely to find the optimal parameter settings, given that the grid covers the true optimal values.\n",
    "    Easy to Parallelize: Can be parallelized across multiple CPUs for efficiency.\n",
    "\n",
    "RandomizedSearchCV:\n",
    "\n",
    "    Random Search: It samples a given number of candidates from a parameter space with a specified distribution. Not every combination is tried but a random subset.\n",
    "    Efficiency: Generally faster than GridSearchCV, as it doesn’t evaluate all combinations but rather a random sample.\n",
    "    Good for Large Spaces: More suitable when the hyperparameter space is large; it can explore the space more efficiently.\n",
    "    Less Precise: Might miss the optimal parameter combination, but often finds a good combination much more quickly than GridSearchCV.\n",
    "    Flexibility in Distributions: Allows specifying distributions for continuous parameters rather than just fixed sets of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Classifier : Same Model Using Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BaggingClassifier\n",
    "\"\"\"\n",
    "Bagging, short for Bootstrap Aggregating, involves training each model in the ensemble using a randomly drawn subset of the training set. \n",
    "It typically uses the same type of algorithm for each model, though it's not a strict requirement.\n",
    "\n",
    "For this example:\n",
    "500 decision tree classifier models on the moons data set,\n",
    " where each model is trained using 100 cases randomly sampled from the training data set with replacement (for bagging). \n",
    "\"\"\"\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier # The ensemble library we are using to do the bagging and voting\n",
    "from sklearn.tree import DecisionTreeClassifier # The model we are using\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "X_train, y_train = make_moons(n_samples=100, noise=0.25, random_state=53)\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500, #n-estimators is the number of models we are building (500 decision tress in this example)\n",
    "    max_samples=100, # max_samples is the number of training instance we are using for each model (100 instances in this example)\n",
    "    bootstrap=True, # set to false if you want to use pasting instead of bagging\n",
    "    n_jobs=-1) # Bootstrap means we are sampling with replacement\n",
    "bag_clf.fit(X_train, y_train)\n",
    "\n",
    "X_test = X_train[ : 1]\n",
    "y_pred = bag_clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier for Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.888\n",
      "SVC 0.896\n",
      "VotingClassifier 0.896\n"
     ]
    }
   ],
   "source": [
    "### VotingClassifier\n",
    "\"\"\"This method involves combining conceptually different machine learning classifiers and using a majority vote (hard voting) \n",
    "or the average predicted probabilities (soft voting) to predict the class labels.\"\"\"\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier # This library does the merging of the models and the voting for an ensemble result\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Define our three models\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "# Train all three models using the ensemble library VotingClassifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Evaluate each model's accuracy on the test set\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 1 1 0 1 0 1\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 0\n",
      " 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0]\n",
      "Feature 0: 0.4998378066205991\n",
      "Feature 1: 0.5001621933794009\n",
      "Accruracy Score: 0.94\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\"\"\"\n",
    "Builds a Random Forest with 500 decision trees, each limited to maximum of 16 leaf nodes. \n",
    "The n_jobs parameter tells the Random Forest to use all available CPU cores to train and predict (n_jobs=-1).\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Generate dataset\n",
    "X, y = make_moons(n_samples=200, noise=0.25, random_state=53)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = rnd_clf.predict(X_test) # This gives us the predicted class for each of the 3 rows of data\n",
    "print(y_pred)\n",
    "\n",
    "# Print feature importances\n",
    "for feature_index, importance in enumerate(rnd_clf.feature_importances_):\n",
    "    print(f\"Feature {feature_index}: {importance}\")\n",
    "\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.09064360774291347\n",
      "sepal width (cm) 0.023762474497524597\n",
      "petal length (cm) 0.43087388510645824\n",
      "petal width (cm) 0.45472003265310373\n"
     ]
    }
   ],
   "source": [
    "# Feature Importantance : Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "# Higher scores are better and mean the feature is more important\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA : Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataset to illustrate the concept of feature importance using pca\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# create sample dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size=0.3, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify The Number of Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.45675886 0.28716049]  explained_variance_ratio_\n",
      "Total Explained Variance:  0.7439193430029332\n",
      "\n",
      "Shape Before Transformation:  (700, 10)\n",
      "Shape After Transformation:  (700, 2)\n"
     ]
    }
   ],
   "source": [
    "## Example with specificied number of components\n",
    "pca = PCA(n_components=2) # If n_components is not set then all components are stored and the sum of the ratios is equal to 1.0.\n",
    "\n",
    "X_train_2D = pca.fit_transform(X_train) # Fit the model with X AND apply the dimensionality reduction on X.\n",
    "X_test_2D = pca.transform(X_test) # Apply dimensionality reduction to X_test.\n",
    "\n",
    "#### Important Note: We apply fit_transform to the training set, but we only apply transform to the test set.\n",
    "#### This is because we want to fit the PCA model to the training set and then apply the same transformation to the test set.\n",
    "#### We would also only apply the transform to the CV set if we had one or any new data that we want to predict on.\n",
    "\n",
    "print(pca.explained_variance_ratio_, ' explained_variance_ratio_') # This will be on the training data as that is we did the fit on\n",
    "print(\"Total Explained Variance: \", sum(pca.explained_variance_ratio_)) # This will be on the training data as that is we did the fit on\n",
    "\n",
    "print(\"\\nShape Before Transformation: \", X_train.shape)\n",
    "print(\"Shape After Transformation: \", X_train_2D.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with 95% Explained Variance Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.45675886 0.28716049 0.12490336 0.09566679]  explained_variance_ratio_\n",
      "Total Explained Variance:  0.9644894887222841\n",
      "\n",
      "Shape Before Transformation:  (700, 10)\n",
      "Shape After Transformation:  (700, 4)\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=0.95) # When we input a value less than one the libarary knows we are going for a target ratio\n",
    "\n",
    "X_train_reduced = pca.fit_transform(X_train) # Fit the model with X AND apply the dimensionality reduction on X.\n",
    "X_test_reduced = pca.transform(X_test) # Apply dimensionality reduction to X_test.\n",
    "\n",
    "\n",
    "print(pca.explained_variance_ratio_, ' explained_variance_ratio_')\n",
    "print(\"Total Explained Variance: \", sum(pca.explained_variance_ratio_))\n",
    "\n",
    "print(\"\\nShape Before Transformation: \", X_train.shape)\n",
    "print(\"Shape After Transformation: \", X_train_reduced.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse the Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: When we inverse the transform back to its orginial dimensions, the data is not restored totally.  Some percentage of data will be lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape Before Transformation:  (700, 10)\n",
      "Shape After Transformation:  (700, 4)\n",
      "Shape After Inverse Transformation:  (700, 10)\n"
     ]
    }
   ],
   "source": [
    "X_recovered = pca.inverse_transform(X_train_reduced)\n",
    "\n",
    "print(\"\\nShape Before Transformation: \", X_train.shape)\n",
    "print(\"Shape After Transformation: \", X_train_reduced.shape)\n",
    "print(\"Shape After Inverse Transformation: \", X_recovered.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Explained Variance as a function of the number of dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the explained variance as a function of the number of dimensions\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blob Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data: 2D points\n",
    "X = np.array([[1, 2], [1, 4], [1, 0],\n",
    "              [10, 2], [10, 4], [10, 0]])\n",
    "\n",
    "k = 2 # Number of clusters\n",
    "# Apply K-Means clustering\n",
    "kmeans = KMeans(n_clusters=k, random_state=0).fit(X)\n",
    "\n",
    "# Predicting the cluster labels\n",
    "labels = kmeans.predict(X)\n",
    "\n",
    "# Centroids of the clusters\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "print('kmeans.inertia_: ', kmeans.inertia_)\n",
    "print('kmeans.score(X): ', kmeans.score(X) )\n",
    "print('silhouette_score', silhouette_score(X, labels, metric='euclidean'))\n",
    "\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', marker='o')\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x')\n",
    "plt.title(\"K-Means Clustering\")\n",
    "plt.xlabel(\"X coordinate\")\n",
    "plt.ylabel(\"Y coordinate\")\n",
    "# Add a legend\n",
    "plt.legend([\"Points\", \"Centroids\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Clusters: \", k)\n",
    "print(\"Labels/ Predictions: \", y_pred)\n",
    "print(\"Centroids: \", centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining PCA and K-Means For Customer Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Standard K-Means On Customer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1332: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kmeans.inertia_:  3260.3333333333335\n",
      "kmeans.score(X):  -3260.3333333333335\n",
      "silhouette_score 0.35714922812639216\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "# Sample Data: Annual Income in thousands and Spending Score\n",
    "X = np.array([[25, 79], [34, 35], [40, 55],\n",
    "              [60, 47], [75, 89], [80, 32],\n",
    "              [20, 77], [58, 15], [45, 65],\n",
    "              [55, 50], [40, 20], [43, 60]])\n",
    "\n",
    "n_clusters = 3\n",
    "# Applying K-Means clustering\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(X)\n",
    "labels = kmeans.predict(X)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "print('kmeans.inertia_: ', kmeans.inertia_)\n",
    "print('kmeans.score(X): ', kmeans.score(X) )\n",
    "print('silhouette_score', silhouette_score(X, labels, metric='euclidean'))\n",
    "\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', marker='o')\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x')\n",
    "plt.title(\"Customer Segmentation using K-Means\")\n",
    "plt.xlabel(\"Annual Income (k$)\")\n",
    "plt.ylabel(\"Spending Score (1-100)\")\n",
    "# Add a legend\n",
    "plt.legend([\"Points\", \"Centroids\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying PCA to reduce the number of dimensions to 2 and then applying K-Means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1332: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kmeans_pca.inertia_:  3260.333333333333\n",
      "kmeans_pca.score(reduced_data):  -3260.333333333333\n",
      "silhouette_score 0.3571492281263921\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Suppose 'X' is your dataset with four features\n",
    "# Apply K-Means clustering (assuming you've already done this)\n",
    "\n",
    "########     PCA     ########\n",
    "\n",
    "# Use PCA to reduce the dataset to 2 dimensions for visualization\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(X)\n",
    "# if we have a test set we would only apply the transform (not fit and transform) to the test set\n",
    "# reduced_data_test = pca.transform(X_test)\n",
    "\n",
    "#########   K-Means    ########\n",
    "\n",
    "# Apply K-Means on the reduced data\n",
    "kmeans_pca = KMeans(n_clusters=3, random_state=0).fit(reduced_data)\n",
    "labels_pca = kmeans_pca.predict(reduced_data)\n",
    "centroids_pca = kmeans_pca.cluster_centers_\n",
    "\n",
    "print('kmeans_pca.inertia_: ', kmeans_pca.inertia_)\n",
    "print('kmeans_pca.score(reduced_data): ', kmeans_pca.score(reduced_data))\n",
    "print('silhouette_score', silhouette_score(reduced_data, labels_pca, metric='euclidean'))\n",
    "\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=labels_pca, cmap='viridis', marker='o')\n",
    "plt.scatter(centroids_pca[:, 0], centroids_pca[:, 1], c='red', marker='x')\n",
    "plt.title(\"Customer Segmentation with PCA\")\n",
    "plt.xlabel(\"PCA Feature 1\")\n",
    "plt.ylabel(\"PCA Feature 2\")\n",
    "plt.legend([\"Points\", \"Centroids\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting error/ accuracy vs number of epochs\n",
    "\n",
    "Sometimes also referred to as error over time.\n",
    "\n",
    "Can use for any model that outputs an error for each epoch which definitely includes those done from scratch\n",
    "\n",
    "y axis is the error\n",
    "x axis is the number of epochs\n",
    "\n",
    "So we see how the error falls as we run more iterations.\n",
    "It also shows how quickly and how smoothly the model converges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.1714497703163476,\n",
       " 1.1274429632405703,\n",
       " 1.085163382576289,\n",
       " 1.044768479482754,\n",
       " 1.00640039131089]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get error/ loss values for sklearn MLPClassifier (Classification NeuralNetwork)\n",
    "loss_values = nn1.loss_curve_\n",
    "loss_values[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Access the loss_curve_ attribute to retrieve the loss at each iteration\n",
    "loss_values = nn1.loss_curve_\n",
    "\n",
    "# Plot the loss function over epochs\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_values, label='Training Loss')\n",
    "plt.title('Error as a function of the epoch')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (915319529.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [38]\u001b[1;36m\u001b[0m\n\u001b[1;33m    if problem == 'regression':\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "max_iter = 250\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for iteration in range(1, max_iter+1):\n",
    "\n",
    "    if iteration % 50 == 0:\n",
    "        print(f\"Iteration: {iteration}\")\n",
    "    #random_state = 10 * run_number\n",
    "\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(20,20), \n",
    "                            learning_rate_init=0.01, \n",
    "                            solver='sgd', \n",
    "                            activation = 'relu',\n",
    "                            max_iter=iteration, \n",
    "                            random_state=60\n",
    "                            )\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "\n",
    "    # Accuracy\n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    \n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, max_iter + 1), train_accuracies, label='Training Accuracy')\n",
    "plt.plot(range(1, max_iter + 1), test_accuracies, label='Test Accuracy')\n",
    "plt.title('Accuracy vs. Number of Iterations for MLPClassifier')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('bias_variance_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tensor flow\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Fit model : This runs the models and stores the results we want to plot in the history variable\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=4000, verbose=0)\n",
    "\n",
    "# Evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0) # The two method outputs are loss and accuracy (if we define a metric as other than accuracy, then metric fills the second variable)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "\n",
    "# Plot history\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.savefig('nodp.png')\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
