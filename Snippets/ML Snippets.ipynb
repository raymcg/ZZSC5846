{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML Snippets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = [[1,2,3],[4,5,6],[7,8,9]]\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.23691392, 1.26598139, 1.81292007, 2.01415931, 1.87153602,\n",
       "        2.98562405],\n",
       "       [0.75445696, 0.95736585, 0.2428874 , 1.54887755, 2.31114727,\n",
       "        0.82717088],\n",
       "       [0.90697559, 0.58073798, 2.64198229, 2.02459886, 1.77226636,\n",
       "        0.41312762],\n",
       "       [2.82179916, 0.50143636, 2.56065491, 0.60651663, 1.71974766,\n",
       "        2.88321456],\n",
       "       [0.33660019, 1.62150803, 2.79162318, 1.276954  , 1.74943718,\n",
       "        1.245788  ],\n",
       "       [1.36761935, 2.86867982, 1.72544758, 1.08510106, 2.10606114,\n",
       "        0.67789981]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.random.rand(6,6)*3\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps and Order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import Data\n",
    "2. Clean / Reformat Data\n",
    "3. Define X and Y\n",
    "4. Normalize X\n",
    "5. Train/ Test/ CV Split\n",
    "6. Train Model, Run K-folds, Test Set Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data[:, :-1], data[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read text file via genfromtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import * # Needed for genfromtxt\n",
    "\n",
    "# data_in = genfromtxt(fpath+'abalone.csv', delimiter=\",\") # in case of csv data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 60/40 train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                    test_size=0.40, random_state=3)\n",
    "\n",
    "# The random state is a seed value for the random number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5) (3,)\n",
      "[[0.75445696 0.95736585 0.2428874  1.54887755 2.31114727]\n",
      " [1.23691392 1.26598139 1.81292007 2.01415931 1.87153602]\n",
      " [0.90697559 0.58073798 2.64198229 2.02459886 1.77226636]]\n",
      "[0.82717088 2.98562405 0.41312762]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize / Scale Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.132e-04 -4.164e-02 -1.720e-01  1.165e+00 -2.318e-01]\n",
      " [-6.166e-01 -4.275e-01 -1.976e+00  2.433e-01  1.800e+00]\n",
      " [-4.218e-01 -8.985e-01  7.808e-01  1.186e+00 -6.906e-01]\n",
      " [ 2.023e+00 -9.976e-01  6.873e-01 -1.623e+00 -9.332e-01]\n",
      " [-1.150e+00  4.029e-01  9.528e-01 -2.953e-01 -7.960e-01]\n",
      " [ 1.663e-01  1.962e+00 -2.725e-01 -6.753e-01  8.520e-01]]\n"
     ]
    }
   ],
   "source": [
    "##### Normalize Input Data #######\n",
    "# Normalize data using min max scaler\n",
    "## We only normalize X, not X and y\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "### OR #### \n",
    "# Normalize data using standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform data \n",
    "#rescaledX = scaler.fit_transform(X) # this applies the scaler to the data and gives the mormalized data\n",
    "scaler.fit(X) # This first out scale to the data (where can then use it later to apply it to other data such as the test data)\n",
    "rescaledX = scaler.transform(X) # this applies the scaler to the data and gives the mormalized data\n",
    "\n",
    "np.set_printoptions(precision = 3) #Setting precision for the output\n",
    "print(rescaledX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting Normalization for Test Data / New Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It IS important that we normalize our X_test and X values that we may subsequently want to use for prediction.\n",
    "Because or weights depend on the scaling/ normalization used for training, we need to ensure it is consistent.\n",
    "This means we should use the same values used for normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized New Data Point: [[ 5.135  7.371  2.447 23.159 25.687]]\n"
     ]
    }
   ],
   "source": [
    "# Create a StandardScaler instance and fit it to the training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train) # Note that this is only fitting the scaler to the training data, it is not scaling the training data\n",
    "# Tranform is the process that scales the data\n",
    "\n",
    "# New data point\n",
    "new_data_point = np.array([[2,3,4,7.0, 8.0]])  # Example new data\n",
    "\n",
    "# Normalize the new data point using the same scaler\n",
    "normalized_new_data_point = scaler.transform(new_data_point) # Here we are transforming our data using the scaler that we fit to the training data\n",
    "\n",
    "print(\"Normalized New Data Point:\", normalized_new_data_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this method, all the values that are above the threshold are transformed into 1 and those equal to or below the threshold are transformed into 0. This method is useful when we deal with probabilities and need to convert the data into crisp values. \n",
    "\n",
    "Binarization is a common operation on text count data where the analyst can decide to only consider the presence or absence of a feature rather than a quantified number of occurrences for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3 -2 -1  0  1  2  3]]\n"
     ]
    }
   ],
   "source": [
    "sample_vector = np.array([-3,-2,-1,0,1,2,3]) # Create sample vector \n",
    "# reshape sample_vector into a 1x7 matrix\n",
    "sample_vector = sample_vector.reshape(1,-1)\n",
    "print(sample_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple NP Method \n",
    "threshold = 0  # Define your threshold here\n",
    "binary_y_train = (y_train > threshold).astype(int)\n",
    "binary_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector before binarization: [[-3 -2 -1  0  1  2  3]]\n",
      "Vector after binarization: [[0 0 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Binarizer.html\n",
    "# NOTE: Input needs to be a matrix, not a vector, so you should reshape any vectors with X.reshape(1, -1)\n",
    "\n",
    "binarizer = Binarizer(threshold=0.0).fit(sample_vector) \n",
    "binaryX = binarizer.transform(sample_vector) \n",
    "\n",
    "print(f\"Vector before binarization: {sample_vector}\")\n",
    "print(f\"Vector after binarization: {binaryX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring your y values are binary given its a classification problem\n",
    "binarizer = Binarizer(threshold=0.0).fit(y_train.reshape(-1,1)) \n",
    "binary_y_train = binarizer.transform(y_train.reshape(-1,1)) # Need to reshape as binarizer only takes 2D arrays\n",
    "binary_y_test = binarizer.transform(y_test.reshape(-1,1)) \n",
    "\n",
    "#convert binary y back to 1D array\n",
    "binary_y_train = binary_y_train.ravel()\n",
    "binary_y_test = binary_y_test.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Vector of Numbers to Be A Matrix For Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1,  2,  3,  4],\n",
       "        [ 2,  3,  4,  5],\n",
       "        [ 3,  4,  5,  6],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 5,  6,  7,  8],\n",
       "        [ 6,  7,  8,  9],\n",
       "        [ 7,  8,  9, 10],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [ 9, 10, 11, 12],\n",
       "        [10, 11, 12, 13]]),\n",
       " array([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15])\n",
    "\n",
    "# convert X to be a matrix of 5 columns (5 days of data)\n",
    "X = np.array([data[i:i+5] for i in range(len(data)-5)])\n",
    "# make X equal to the first 4 columns of X and y equal to the last column of X\n",
    "X, y = X[:,:-1], X[:,-1]\n",
    "X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv_results: {'fit_time': array([0.00033951, 0.        , 0.00099778, 0.00099778, 0.        ]), 'score_time': array([0., 0., 0., 0., 0.]), 'test_score': array([0.29828675, 0.2241492 , 0.15480127, 0.25519733, 0.17108715])}\n",
      "\n",
      "scores['test_neg_mean_squared_error']=array([-2807.16799618, -4890.37813089, -3360.65006947, -4663.03492141,\n",
      "       -5152.31964346])\n",
      "scores['train_r2']=array([0.32756806, 0.31459442, 0.33970628, 0.32065301, 0.27361929])\n",
      "\n",
      "cv_results: {'fit_time': array([0.00033951, 0.        , 0.00099778, 0.00099778, 0.        ]), 'score_time': array([0., 0., 0., 0., 0.]), 'test_score': array([0.29828675, 0.2241492 , 0.15480127, 0.25519733, 0.17108715])}\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    " \n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn import metrics  \n",
    "\n",
    "\n",
    "#Source: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html\n",
    "#Simulate splitting a dataset  into 5 folds\n",
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data[:150]\n",
    "\n",
    "y = diabetes.target[:150]\n",
    "lasso = linear_model.Lasso()\n",
    "\n",
    "#############################################\n",
    "#Single metric evaluation using cross_validate\n",
    "cv_results = cross_validate(lasso, X, y, cv=5)\n",
    "print(f\"cv_results: {cv_results}\\n\")\n",
    "\n",
    "scores = cross_validate(lasso, X, y, cv=5, scoring=('r2', 'neg_mean_squared_error'), return_train_score=True)\n",
    "print(f\"{scores['test_neg_mean_squared_error']=}\") \n",
    "print(f\"{scores['train_r2']=}\\n\")\n",
    "print(f\"cv_results: {cv_results}\")\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold cv for tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "Score for fold 1: loss of -697.5126953125; accuracy of 0.0%\n",
      "Training on fold 2...\n",
      "Score for fold 2: loss of -433.4169616699219; accuracy of 0.0%\n",
      "Training on fold 3...\n",
      "Score for fold 3: loss of -350.8194580078125; accuracy of 0.0%\n",
      "Training on fold 4...\n",
      "Score for fold 4: loss of -359.3729553222656; accuracy of 0.0%\n",
      "Training on fold 5...\n",
      "Score for fold 5: loss of -683.8095703125; accuracy of 0.0%\n",
      "Test set evaluation - Loss: -870.7814331054688, Accuracy: 0.0%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# First, split your data into a training and a hold-out test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to create model, required for KFold\n",
    "def create_model(input_dim):\n",
    "    # Define the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Prepare cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "##################################################\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for train, val in kfold.split(X_train, y_train):\n",
    "    # train is an array of indices of the datapoints to train on\n",
    "    # val is an array of indices of the datapoints for cross validation \n",
    "#################################################\n",
    "    model = create_model(input_dim=X_train.shape[1])\n",
    "    print(f'Training on fold {fold_no}...')\n",
    "\n",
    "    # Fit the model\n",
    "    history = model.fit(X_train[train], y_train[train], # selecting those index values from X_train and y_train to use as TRAINING data\n",
    "                        validation_data=(X_train[val], y_train[val]), # selecting those index values from X_train and y_train to use as VALIDATION data\n",
    "                        epochs=100,\n",
    "                        verbose=0)\n",
    "    \n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(X_train[val], y_train[val], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    fold_no += 1\n",
    "\n",
    "# Finally, after choosing and training your final model, evaluate it on the test set\n",
    "final_model = create_model(input_dim=X_train.shape[1])\n",
    "final_model.fit(X_train, y_train, epochs=100, verbose=0)\n",
    "test_scores = final_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test set evaluation - Loss: {test_scores[0]}, Accuracy: {test_scores[1]*100}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `kfold` is an instance of `KFold` from `sklearn.model_selection`, which is configured to split the data into a certain number of folds.\n",
    "- `kfold.split(X_train, y_train)` returns an iterator that generates pairs of indices. These pairs are the indices for the training set and the validation set for each fold.\n",
    "- Each `train` and `val` in the `for` loop are arrays of indices. `train` contains the indices of `X_train` and `y_train` that are going into making up the training set for the current fold, while `val` contains the indices of `X_train` and `y_train` that make up the validation set for the current fold.\n",
    "\n",
    "When you use `X_train[train]` and `y_train[train]`, you are indexing `X_train` and `y_train` with the `train` indices to extract the data points that will be used for training the model in the current fold.\n",
    "\n",
    "Similarly, `X_train[val]` and `y_train[val]` index `X_train` and `y_train` with the `val` indices to extract the data points that will be used for validating the model in the current fold.\n",
    "\n",
    "The `model.fit()` function then takes these indexed portions of the data to train the model and validate its performance.\n",
    "\n",
    "So, in summary:\n",
    "- `train` is an array of indices for the training data in the current fold.\n",
    "- `val` is an array of indices for the validation data in the current fold.\n",
    "- `X_train[train]` and `y_train[train]` are the subsets of the data used for training.\n",
    "- `X_train[val]` and `y_train[val]` are the subsets of the data used for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy and Results Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [1.1, 2, 1, 4]\n",
    "y_pred = [1.1, 1.8, 1.2, 3.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.02\n",
      "R-squared (R2): 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Performance metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R-squared (R2): {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn library for classification accuracy metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [0,0,1,1]\n",
    "y_pred = [0,0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[2 0]\n",
      " [1 1]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80         2\n",
      "           1       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.75         4\n",
      "   macro avg       0.83      0.75      0.73         4\n",
      "weighted avg       0.83      0.75      0.73         4\n",
      "\n",
      "Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Performance metrics\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# if we want to access the indiv accuracy scores : see below for more details\n",
    "report_dict = classification_report(y_test, y_pred, output_dict =True) \n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "## for our ROC curve we need to generate the y_pred probabilities from our model\n",
    "# from sklearn.metrics import roc_curve, auc\n",
    "# Detailed example below\n",
    "# y_pred_proba = model.predict_proba(X_test)[:,1] # This gives us the probabilities for the positive class only\n",
    "# roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "#print(f\"ROC-AUC: {roc_auc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0]\n",
      " [1 1]]\n",
      "True Negatives (TN): 2\n",
      "False Positives (FP): 0\n",
      "False Negatives (FN): 1\n",
      "True Positives (TP): 1\n"
     ]
    }
   ],
   "source": [
    "y_test = [0,0,1,1]\n",
    "y_pred = [0,0,0,1]\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrix)\n",
    "\n",
    "# Extract TN, FP, FN, TP\n",
    "TN, FP, FN, TP = conf_matrix.ravel()\n",
    "# The order of the output from sklearn is usually TN, FP, FN, TP\n",
    "\n",
    "# Print the results\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")\n",
    "print(f\"True Positives (TP): {TP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80         2\n",
      "           1       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.75         4\n",
      "   macro avg       0.83      0.75      0.73         4\n",
      "weighted avg       0.83      0.75      0.73         4\n",
      "\n",
      "\n",
      "Recall for class 0: 1.0\n",
      "Recall for class 1: 0.5\n",
      "Accuracy: 0.75\n",
      "F1-Score for class 0: 0.8\n",
      "F1-Score for class 1: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "y_test = [0, 0, 1, 1]  # True labels\n",
    "y_pred = [0, 0, 0, 1]  # Predicted labels\n",
    "\n",
    "# Generate the classification report : setting output_dict to True will return a dictionary of the classification report\n",
    "report_dict = classification_report(y_test, y_pred, output_dict =True)\n",
    "\n",
    "# Extract recall values for each class\n",
    "recall_0 = report_dict['0']['recall']\n",
    "recall_1 = report_dict['1']['recall']\n",
    "\n",
    "# Extract accuracy and f1-score\n",
    "accuracy = report_dict['accuracy']\n",
    "f1_score_0 = report_dict['0']['f1-score']\n",
    "f1_score_1 = report_dict['1']['f1-score']\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print()\n",
    "# Print the results\n",
    "print(f\"Recall for class 0: {recall_0}\")\n",
    "print(f\"Recall for class 1: {recall_1}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1-Score for class 0: {f1_score_0}\")\n",
    "print(f\"F1-Score for class 1: {f1_score_1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC_AUC Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(5,), max_iter=1000, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(5,), max_iter=1000, random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(5,), max_iter=1000, random_state=1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a synthetic binary classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=100, n_features=4, n_classes=2, random_state=1)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Create and train the neural network model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "model = MLPClassifier(hidden_layer_sizes=(5,), max_iter=1000, random_state=1)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 1.00\n"
     ]
    }
   ],
   "source": [
    "## for our ROC curve we need to generate the y_pred probabilities from our model\n",
    "\n",
    "y_pred_proba = model.predict_proba(X_test)[:,1] # This gives us the probabilities for the positive class only\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"ROC-AUC: {roc_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Results Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Fit the Model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=4000, verbose=0)\n",
    "\n",
    "# Evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "\n",
    "history.history['accuracy'] ## history is the parameter storing the results of the model run: tf records results for both the train and validation data\n",
    "history.history['val_accuracy'] ## history is the parameter storing the results of the model run: : tf records results for both the train and validation data\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGULARIZATION - MISSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three different ways to define a model using regularizers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_l1_LR = LogisticRegression(C=0.5, penalty='l1', tol=0.01, solver='saga')\n",
    "clf_l2_LR = LogisticRegression(C=0.5, penalty='l2', tol=0.01, solver='saga')\n",
    "clf_en_LR = LogisticRegression(C=0.5, penalty='elasticnet', solver='saga',\n",
    "                                   l1_ratio=0.5, tol=0.01)\n",
    "\n",
    "# C is the inverse of the regularization strength, smaller values specify stronger regularization\n",
    "# tol is the tolerance for stopping criteria (when loss is low enought to stop)\n",
    "# saga is an extension of sgd and supports l1 and l2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 Regularization with Tensor Flow\n",
    "from keras.regularizers import l2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(500, input_dim=2, activation='relu', kernel_regularizer= l2(0.001))) #### This is the line we add the regularizer to\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELS - Only Partially Completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "\n",
    "# import mlp regression model\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solver = 'sgd' # Stochastic Gradient Descent\n",
    "solver = 'adam' # Adam\n",
    "activation = 'relu' # Rectified Linear Unit (ReLU)\n",
    "# activation = 'logistic' # Logistic (sigmoid)\n",
    "alpha = 0.0001 # L2 penalty (regularization term) parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape=(70, 4), X_test.shape=(30, 4)\n",
      "Training set predictions:\n",
      " [1 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 0 0 1 1 1 0 0 1 1\n",
      " 0 0 0 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 0 0 1 0]\n",
      "Test set predictions:\n",
      " [0 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 0 1 1 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1 Hidden layer\n",
    "nn1 = MLPClassifier(hidden_layer_sizes=(8,), random_state=2, max_iter=100, activation = activation, solver=solver,  \n",
    "                alpha = alpha, learning_rate_init=0.01 )\n",
    "        #https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "\n",
    "# 2 Hidden layers\n",
    "nn2 = MLPClassifier(hidden_layer_sizes=(8,4), random_state=2, max_iter=100,activation = activation, solver=solver,\n",
    "                alpha = alpha, learning_rate='constant', learning_rate_init=0.01)\n",
    "        #hidden_layer_sizes=(hidden,hidden, hidden) would implement 3 hidden layers\n",
    " \n",
    "# Train the model using the training sets\n",
    "nn1.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred_test = nn1.predict(X_test)\n",
    "y_pred_train = nn1.predict(X_train)\n",
    "\n",
    "print(f\"{X_train.shape=}, {X_test.shape=}\")\n",
    "print(f\"Training set predictions:\\n {y_pred_train}\")\n",
    "print(f\"Test set predictions:\\n {y_pred_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKLearn Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Hidden layer\n",
    "nn1 = MLPRegressor(hidden_layer_sizes=(8,), random_state=2, max_iter=1000, activation = 'relu', solver='adam',  \n",
    "                alpha = 0.1, learning_rate_init=0.01, tol=0.01 )\n",
    "        #https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html\n",
    "\n",
    "# Train the model using the training sets\n",
    "nn1.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred_test = nn1.predict(X_test)\n",
    "y_pred_train = nn1.predict(X_train)\n",
    "\n",
    "print(f\"{X_train.shape=}, {X_test.shape=}\")\n",
    "print(f\"Training set predictions:\\n {y_pred_train}\")\n",
    "print(f\"Test set predictions:\\n {y_pred_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 25)                125       \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 26        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 151\n",
      "Trainable params: 151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.9624 - accuracy: 0.3857 - val_loss: 0.8870 - val_accuracy: 0.4000\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.9404 - accuracy: 0.3857 - val_loss: 0.8707 - val_accuracy: 0.4000\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.9214 - accuracy: 0.3857 - val_loss: 0.8550 - val_accuracy: 0.4000\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.9020 - accuracy: 0.3714 - val_loss: 0.8398 - val_accuracy: 0.4000\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.8831 - accuracy: 0.3714 - val_loss: 0.8249 - val_accuracy: 0.4000\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.8653 - accuracy: 0.3714 - val_loss: 0.8103 - val_accuracy: 0.4000\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.8477 - accuracy: 0.3714 - val_loss: 0.7961 - val_accuracy: 0.4000\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.8314 - accuracy: 0.3714 - val_loss: 0.7825 - val_accuracy: 0.4000\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.8145 - accuracy: 0.3714 - val_loss: 0.7694 - val_accuracy: 0.4000\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.7988 - accuracy: 0.3714 - val_loss: 0.7565 - val_accuracy: 0.4333\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.7837 - accuracy: 0.3571 - val_loss: 0.7440 - val_accuracy: 0.4000\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.7694 - accuracy: 0.3571 - val_loss: 0.7319 - val_accuracy: 0.4333\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.7551 - accuracy: 0.3571 - val_loss: 0.7202 - val_accuracy: 0.4667\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.7406 - accuracy: 0.4000 - val_loss: 0.7087 - val_accuracy: 0.4667\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.7277 - accuracy: 0.4000 - val_loss: 0.6978 - val_accuracy: 0.4667\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.7143 - accuracy: 0.4143 - val_loss: 0.6876 - val_accuracy: 0.4667\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.7019 - accuracy: 0.4286 - val_loss: 0.6777 - val_accuracy: 0.5000\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6900 - accuracy: 0.4429 - val_loss: 0.6679 - val_accuracy: 0.5333\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6788 - accuracy: 0.4571 - val_loss: 0.6580 - val_accuracy: 0.5333\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6669 - accuracy: 0.4857 - val_loss: 0.6482 - val_accuracy: 0.5333\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6550 - accuracy: 0.5000 - val_loss: 0.6385 - val_accuracy: 0.5333\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6440 - accuracy: 0.5429 - val_loss: 0.6289 - val_accuracy: 0.5333\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6330 - accuracy: 0.5286 - val_loss: 0.6194 - val_accuracy: 0.5000\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6216 - accuracy: 0.5000 - val_loss: 0.6103 - val_accuracy: 0.5000\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.6111 - accuracy: 0.5286 - val_loss: 0.6012 - val_accuracy: 0.5667\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6008 - accuracy: 0.5857 - val_loss: 0.5923 - val_accuracy: 0.6667\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5913 - accuracy: 0.6286 - val_loss: 0.5836 - val_accuracy: 0.7333\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5816 - accuracy: 0.6857 - val_loss: 0.5751 - val_accuracy: 0.8000\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5725 - accuracy: 0.7429 - val_loss: 0.5669 - val_accuracy: 0.9000\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5633 - accuracy: 0.8286 - val_loss: 0.5588 - val_accuracy: 0.9333\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5545 - accuracy: 0.8857 - val_loss: 0.5508 - val_accuracy: 0.9333\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5455 - accuracy: 0.9286 - val_loss: 0.5429 - val_accuracy: 0.9333\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5371 - accuracy: 0.9286 - val_loss: 0.5352 - val_accuracy: 0.9333\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5285 - accuracy: 0.9714 - val_loss: 0.5275 - val_accuracy: 0.9333\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5200 - accuracy: 0.9714 - val_loss: 0.5199 - val_accuracy: 0.9333\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5114 - accuracy: 0.9714 - val_loss: 0.5122 - val_accuracy: 0.9333\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5029 - accuracy: 0.9714 - val_loss: 0.5046 - val_accuracy: 0.9333\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4948 - accuracy: 0.9714 - val_loss: 0.4972 - val_accuracy: 0.9333\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4864 - accuracy: 0.9714 - val_loss: 0.4898 - val_accuracy: 0.9333\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4783 - accuracy: 0.9714 - val_loss: 0.4825 - val_accuracy: 0.9333\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4705 - accuracy: 0.9714 - val_loss: 0.4755 - val_accuracy: 0.9333\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4631 - accuracy: 0.9714 - val_loss: 0.4687 - val_accuracy: 0.9333\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4554 - accuracy: 0.9714 - val_loss: 0.4620 - val_accuracy: 0.9333\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4483 - accuracy: 0.9571 - val_loss: 0.4554 - val_accuracy: 0.9333\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4413 - accuracy: 0.9571 - val_loss: 0.4488 - val_accuracy: 0.9333\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4344 - accuracy: 0.9714 - val_loss: 0.4425 - val_accuracy: 0.9333\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4275 - accuracy: 0.9714 - val_loss: 0.4363 - val_accuracy: 0.9333\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4208 - accuracy: 0.9714 - val_loss: 0.4303 - val_accuracy: 0.9333\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4139 - accuracy: 0.9714 - val_loss: 0.4243 - val_accuracy: 0.9333\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4071 - accuracy: 0.9714 - val_loss: 0.4184 - val_accuracy: 0.9333\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4005 - accuracy: 0.9714 - val_loss: 0.4125 - val_accuracy: 0.9333\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3940 - accuracy: 0.9714 - val_loss: 0.4067 - val_accuracy: 0.9333\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3875 - accuracy: 0.9714 - val_loss: 0.4009 - val_accuracy: 0.9333\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3811 - accuracy: 0.9714 - val_loss: 0.3950 - val_accuracy: 0.9333\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3748 - accuracy: 0.9714 - val_loss: 0.3892 - val_accuracy: 0.9333\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3687 - accuracy: 0.9714 - val_loss: 0.3837 - val_accuracy: 0.9333\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3626 - accuracy: 0.9714 - val_loss: 0.3782 - val_accuracy: 0.9333\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3567 - accuracy: 0.9714 - val_loss: 0.3726 - val_accuracy: 0.9333\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3509 - accuracy: 0.9714 - val_loss: 0.3672 - val_accuracy: 0.9333\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3451 - accuracy: 0.9714 - val_loss: 0.3619 - val_accuracy: 0.9333\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3395 - accuracy: 0.9714 - val_loss: 0.3567 - val_accuracy: 0.9333\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3337 - accuracy: 0.9714 - val_loss: 0.3516 - val_accuracy: 0.9333\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3280 - accuracy: 0.9714 - val_loss: 0.3466 - val_accuracy: 0.9333\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3224 - accuracy: 0.9714 - val_loss: 0.3417 - val_accuracy: 0.9333\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3169 - accuracy: 0.9714 - val_loss: 0.3369 - val_accuracy: 0.9333\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3115 - accuracy: 0.9714 - val_loss: 0.3322 - val_accuracy: 0.9333\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3063 - accuracy: 0.9857 - val_loss: 0.3275 - val_accuracy: 0.9333\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3009 - accuracy: 0.9857 - val_loss: 0.3228 - val_accuracy: 0.9333\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2958 - accuracy: 0.9857 - val_loss: 0.3182 - val_accuracy: 0.9333\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.2906 - accuracy: 0.9857 - val_loss: 0.3137 - val_accuracy: 0.9333\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2857 - accuracy: 0.9857 - val_loss: 0.3092 - val_accuracy: 0.9333\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2810 - accuracy: 0.9857 - val_loss: 0.3049 - val_accuracy: 0.9333\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2761 - accuracy: 0.9857 - val_loss: 0.3008 - val_accuracy: 0.9333\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2716 - accuracy: 0.9857 - val_loss: 0.2966 - val_accuracy: 0.9333\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2669 - accuracy: 0.9857 - val_loss: 0.2925 - val_accuracy: 0.9333\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2623 - accuracy: 0.9857 - val_loss: 0.2886 - val_accuracy: 0.9333\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2579 - accuracy: 0.9857 - val_loss: 0.2849 - val_accuracy: 0.9333\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2535 - accuracy: 0.9857 - val_loss: 0.2814 - val_accuracy: 0.9333\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2492 - accuracy: 0.9857 - val_loss: 0.2779 - val_accuracy: 0.9333\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2452 - accuracy: 0.9857 - val_loss: 0.2743 - val_accuracy: 0.9333\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2411 - accuracy: 0.9857 - val_loss: 0.2708 - val_accuracy: 0.9333\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2371 - accuracy: 0.9857 - val_loss: 0.2673 - val_accuracy: 0.9333\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2332 - accuracy: 0.9857 - val_loss: 0.2639 - val_accuracy: 0.9333\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2296 - accuracy: 0.9857 - val_loss: 0.2606 - val_accuracy: 0.9333\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2259 - accuracy: 0.9857 - val_loss: 0.2573 - val_accuracy: 0.9333\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2222 - accuracy: 0.9857 - val_loss: 0.2539 - val_accuracy: 0.9333\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2185 - accuracy: 0.9857 - val_loss: 0.2505 - val_accuracy: 0.9333\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2148 - accuracy: 0.9857 - val_loss: 0.2472 - val_accuracy: 0.9333\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2112 - accuracy: 0.9857 - val_loss: 0.2440 - val_accuracy: 0.9333\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2077 - accuracy: 0.9857 - val_loss: 0.2409 - val_accuracy: 0.9333\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2043 - accuracy: 0.9857 - val_loss: 0.2379 - val_accuracy: 0.9333\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2010 - accuracy: 0.9857 - val_loss: 0.2349 - val_accuracy: 0.9333\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.1977 - accuracy: 0.9857 - val_loss: 0.2323 - val_accuracy: 0.9333\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1946 - accuracy: 0.9857 - val_loss: 0.2297 - val_accuracy: 0.9333\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1916 - accuracy: 0.9857 - val_loss: 0.2272 - val_accuracy: 0.9333\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1885 - accuracy: 0.9857 - val_loss: 0.2248 - val_accuracy: 0.9333\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1857 - accuracy: 0.9857 - val_loss: 0.2224 - val_accuracy: 0.9333\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1828 - accuracy: 0.9857 - val_loss: 0.2200 - val_accuracy: 0.9333\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1799 - accuracy: 0.9857 - val_loss: 0.2177 - val_accuracy: 0.9333\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1771 - accuracy: 0.9857 - val_loss: 0.2154 - val_accuracy: 0.9333\n",
      "\n",
      "Train Accuracy: 0.986, Test Accuracy: 0.933\n"
     ]
    }
   ],
   "source": [
    "def build_tf_model():\n",
    "    # Define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, input_dim=X_train.shape[1], activation='relu')) # Hidden layer 1: 25 neurons, shows that the input layer has X_train.shape[1] neurons\n",
    "    model.add(Dense(1, activation='sigmoid')) # Output layer: 1 neuron, sigmoid activation\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) # Build the model (build it, not run it)\n",
    "\n",
    "    model.summary() # Print model summary\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_tf_model()\n",
    "\n",
    "# Fit model : Runs the model and outputs the results into history\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "_, train_acc = model.evaluate(X_train, y_train, verbose=0) # The two method outputs are loss and accuracy (accuracy because we defined that as our metric)\n",
    "_, test_acc = model.evaluate(X_test, y_test, verbose=0) # The two method outputs are loss and accuracy (accuracy because we defined that as our metric)\n",
    "print('\\nTrain Accuracy: %.3f, Test Accuracy: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees : scikitlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.tree import export_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_depth = 20\n",
    "min_in_leaf = 1\n",
    "#classifier =  DecisionTreeClassifier(random_state=0, max_depth=tree_depth)\n",
    "classifier =  DecisionTreeClassifier(max_features=\"auto\", random_state=25, min_samples_leaf=min_in_leaf, max_depth=tree_depth)\n",
    "\n",
    "# train model \n",
    "decision_tree = classifier.fit(features_train, target_train)\n",
    "\n",
    "#plot_tree(decision_tree) \n",
    "r = export_text(decision_tree, show_weights=True, feature_names= list(ff.columns)) \n",
    "print(r)\n",
    " \n",
    "# now make predictions\n",
    "target_predicted = decision_tree.predict(features_test)\n",
    "\n",
    "# create confusion matrix\n",
    "matrix = confusion_matrix(target_test, target_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=0,\n",
    "                           random_state=42, n_clusters_per_class=1)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the AdaBoost classifier\n",
    "# Using default settings: decision tree stumps as weak learners\n",
    "ada_clf = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "\"\"\"\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    base_estimator=decision_tree_model, n_estimators=10,\n",
    "    algorithm=\"SAMME.R\", learning_rate= 1)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "ada_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = ada_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters\n",
    "\n",
    "1. **base_estimator**: This defines the type of weak learner to use. By default, AdaBoost uses decision tree stumps (i.e., `DecisionTreeClassifier(max_depth=1)`). However, you can specify any other model as the base estimator.\n",
    "\n",
    "2. **n_estimators**: This parameter sets the number of weak learners to be used in the boosting process. A higher number of estimators can improve the model's performance but also increases the risk of overfitting and computational cost.\n",
    "\n",
    "3. **learning_rate**: This parameter shrinks the contribution of each classifier. There is a trade-off between learning_rate and n_estimators. A smaller value of learning_rate requires a larger number of n_estimators for the same level of accuracy.\n",
    "\n",
    "4. **algorithm**: AdaBoost supports two algorithms: `SAMME` and `SAMME.R`. The `SAMME.R` algorithm typically converges faster than `SAMME`, needing fewer boosting iterations. `SAMME.R` uses the class probabilities whereas `SAMME` uses classifications.\n",
    "\n",
    "5. **random_state**: This controls the randomness of the algorithm. Providing a fixed random_state ensures reproducibility of the results.\n",
    "\n",
    "Adjusting these parameters can help tailor the AdaBoost model to your specific dataset and problem. It's often useful to use techniques like grid search or random search for hyperparameter tuning to find the most effective combination of these parameters. \n",
    "\n",
    "Keep in mind that while more complex base estimators and a higher number of estimators might improve model performance, they also increase the risk of overfitting and the computational cost. As with any machine learning model, there's a balance to be struck between complexity, performance, and computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbrt.predict y value :  [0.75026781]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)\n",
    "\n",
    "\n",
    "# Use Gradient Booster to Implement Gradient Boosting\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, \n",
    "                                 learning_rate=1.0, random_state=42)\n",
    "# max_depth is the depth of each of the indiv  trees\n",
    "# n_estimators is how many trees to build\n",
    "\n",
    "model = gbrt.fit(X, y)\n",
    "\n",
    "X_new = np.array([[0.8]])\n",
    "print('gbrt.predict y value : ', gbrt.predict(X_new) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$S(x) =  1 / (1 + e^{-x})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before sigmoid activation: [-3 -2 -1  0  1  2  3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.047, 0.119, 0.269, 0.5  , 0.731, 0.881, 0.953])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_vector = np.array([-3,-2,-1,0,1,2,3]) # Create sample vector\n",
    "\n",
    "print(f\"Before sigmoid activation: {sample_vector}\") # Note: We don't apply sigmoid to y, we apply it to generate y_predicted from our z values\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# All our negative input values will have a transformed value below 0.5    \n",
    "# All our positive inpu values will have a transformed value above 0.5\n",
    "sigmoid(sample_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.002 0.002 0.003 ... 0.997 0.997 0.998]\n",
      "[0.002 0.002 0.003 ... 0.003 0.003 0.002]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    s=1/(1+np.exp(-x)) # This implements the sigmoid function\n",
    "    ds=s*(1-s)  # This gives the derivative \n",
    "    return s,ds\n",
    "    \n",
    "x=np.arange(-6,6,0.01)\n",
    "\n",
    "s, ds = sigmoid(x)\n",
    "print(s)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.718  7.389 20.086 54.598  2.718  7.389 20.086]\n",
      "114.98389973429897\n",
      "[0.024 0.064 0.175 0.475 0.024 0.064 0.175]\n"
     ]
    }
   ],
   "source": [
    "a = [1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0]\n",
    "print(np.exp(a))\n",
    "print(np.sum(np.exp(a)))\n",
    "\n",
    "# exp is the numpy function for the natural log e (so a to the power log e)\n",
    "soft_max = np.exp(a) / np.sum(np.exp(a)) \n",
    "print(soft_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "# model.add(Dropout(0.4)) # Dropout layer with a 40% dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITH DROPOUTS, JUST THE ONE LINE ADDED\n",
    "model = Sequential()\n",
    "model.add(Dense(500, input_dim=2, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# WITH DROPOUTS, JUST THE ONE IS DIFFERENT\n",
    "model = Sequential()\n",
    "model.add(Dense(500, input_dim=2, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of classification problems, you need a proper way to represent the class values or outcomes, especially in multi-class problems.\n",
    "\n",
    "A one-hot encoding is a representation of categorical variables as binary vectors. The categorical values are typically mapped to integer values and then each integer value is represented as a binary vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cold' 'cold' 'warm' 'cold' 'hot' 'hot' 'warm' 'cold' 'warm' 'hot']\n"
     ]
    }
   ],
   "source": [
    "#Source: https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\n",
    "\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Define example\n",
    "data_to_encode = ['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']\n",
    "values = array(data_to_encode)\n",
    "print(values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 2 0 1 1 2 0 2 1]\n"
     ]
    }
   ],
   "source": [
    "#### STEP 1 #### : CONVERT TO INTEGER VALUES\n",
    "\n",
    "# Here we are converting cold to be 0, hot to be 1, and warm to be 2\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Hot Encoded\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "### STEP 2 ### : ONE HOT ENCODE\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False) # sparse=False ensures we get a non-sparse matrix\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1) # Reshape to be a 2D array with 1 column\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print('One Hot Encoded')\n",
    "print(onehot_encoded)\n",
    "\n",
    "# Our first row shows the result for cold, which is 0, so we have [1,0,0]\n",
    "# Our second row shows the result for cold, which is 0, so we have [1,0,0]\n",
    "# Our third row shows the result for warm, which is 2, so we have [0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inverted\n",
      "['cold']\n"
     ]
    }
   ],
   "source": [
    "# Invert first example if we want to work backwards when we get our predicitons\n",
    "inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
    "print('\\nInverted')\n",
    "print(inverted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRIDSEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = genfromtxt(\"C:/Dropbox/Variance/UNSW/ZZSC5836/raw_data/pima.csv\", delimiter=\",\")\n",
    " \n",
    "data_inputx = data_in[:,0:8] # all features 0, 1, 2, 3, 4, 5, 6, 7 \n",
    "data_inputy = data_in[:,-1] # this is target - so that last col is selected from data\n",
    "\n",
    "# split to training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_inputx, data_inputy, test_size=0.40, random_state=23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters discovered:\n",
      " {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "\n",
      "Report results on the test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.81      0.76       197\n",
      "         1.0       0.56      0.42      0.48       111\n",
      "\n",
      "    accuracy                           0.67       308\n",
      "   macro avg       0.64      0.62      0.62       308\n",
      "weighted avg       0.66      0.67      0.66       308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model generation object  \n",
    "mlp = MLPClassifier(max_iter=1000)\n",
    "\n",
    "# Parameter Space\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,), (50,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "    }\n",
    "\n",
    "# Search through parameter space, build models for different parameters\n",
    "clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n",
    "# Grid Search for MLPClassifier as the model, parameter_space as the space of parameters, n_jobs=-1 to use all processors, cv=3 for 3-fold cross validation\n",
    "\n",
    "#clf.fit(X_train, binary_y_train)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# Best parameteR set\n",
    "print('Best parameters discovered:\\n', clf.best_params_)\n",
    "\n",
    "########## EVALUATION AND PREDICTION ##########\n",
    "y_true, y_pred = y_test , clf.predict(x_test)\n",
    "## Very interesting is the clf.predict selects the optimal parameters from the gridsearch cv results\n",
    "\n",
    "\n",
    "print('\\nReport results on the test set:')\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "\n",
    "#acc_test = accuracy_score(y_pred, y_true) \n",
    "\n",
    "y_pred_train = clf.predict(x_train)\n",
    "#acc_train = accuracy_score(y_pred_train, y_train) \n",
    "\n",
    "#cm = confusion_matrix(y_pred, y_true) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV:\n",
    "\n",
    "    Exhaustive Search: It performs an exhaustive search over a specified parameter space. This means it tries every combination of the provided hyperparameter values.\n",
    "    Time-Consuming: As it evaluates all possible combinations, it can be very time-consuming, especially for large datasets and complex models.\n",
    "    Precision: Since it checks all combinations, it’s more likely to find the optimal parameter settings, given that the grid covers the true optimal values.\n",
    "    Easy to Parallelize: Can be parallelized across multiple CPUs for efficiency.\n",
    "\n",
    "RandomizedSearchCV:\n",
    "\n",
    "    Random Search: It samples a given number of candidates from a parameter space with a specified distribution. Not every combination is tried but a random subset.\n",
    "    Efficiency: Generally faster than GridSearchCV, as it doesn’t evaluate all combinations but rather a random sample.\n",
    "    Good for Large Spaces: More suitable when the hyperparameter space is large; it can explore the space more efficiently.\n",
    "    Less Precise: Might miss the optimal parameter combination, but often finds a good combination much more quickly than GridSearchCV.\n",
    "    Flexibility in Distributions: Allows specifying distributions for continuous parameters rather than just fixed sets of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Classifier : Same Model Using Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BaggingClassifier\n",
    "\"\"\"\n",
    "Bagging, short for Bootstrap Aggregating, involves training each model in the ensemble using a randomly drawn subset of the training set. \n",
    "It typically uses the same type of algorithm for each model, though it's not a strict requirement.\n",
    "\n",
    "For this example:\n",
    "500 decision tree classifier models on the moons data set,\n",
    " where each model is trained using 100 cases randomly sampled from the training data set with replacement (for bagging). \n",
    "\"\"\"\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier # The ensemble library we are using to do the bagging and voting\n",
    "from sklearn.tree import DecisionTreeClassifier # The model we are using\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "X_train, y_train = make_moons(n_samples=100, noise=0.25, random_state=53)\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    max_samples=100, bootstrap=True, n_jobs=-1) # Bootstrap means we are sampling with replacement\n",
    "bag_clf.fit(X_train, y_train)\n",
    "\n",
    "X_test = X_train[ : 1]\n",
    "y_pred = bag_clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier for Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.888\n",
      "SVC 0.896\n",
      "VotingClassifier 0.896\n"
     ]
    }
   ],
   "source": [
    "### VotingClassifier\n",
    "\"\"\"This method involves combining conceptually different machine learning classifiers and using a majority vote (hard voting) \n",
    "or the average predicted probabilities (soft voting) to predict the class labels.\"\"\"\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier # This library does the merging of the models and the voting for an ensemble result\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Define our three models\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "# Train all three models using the ensemble library VotingClassifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Evaluate each model's accuracy on the test set\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1]\n",
      "Accruracy Score: 0.896\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\"\"\"\n",
    "Builds a Random Forest with 500 decision trees, each limited to maximum of 16 leaf nodes. \n",
    "The n_jobs parameter tells the Random Forest to use all available CPU cores to train and predict (n_jobs=-1).\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "X_train, y_train = make_moons(n_samples=100, noise=0.25, random_state=53)\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "X_test = X_train[0:3]\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test) # This gives us the predicted class for each of the 3 rows of data\n",
    "print(y_pred_rf)\n",
    "\n",
    "print(f\"Accruracy Score: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.09064360774291347\n",
      "sepal width (cm) 0.023762474497524597\n",
      "petal length (cm) 0.43087388510645824\n",
      "petal width (cm) 0.45472003265310373\n"
     ]
    }
   ],
   "source": [
    "# Feature Importantance : Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "# Higher scores are better and mean the feature is more important\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting error/ accuracy vs number of epochs\n",
    "\n",
    "Sometimes also referred to as error over time.\n",
    "\n",
    "Can use for any model that outputs an error for each epoch which definitely includes those done from scratch\n",
    "\n",
    "y axis is the error\n",
    "x axis is the number of epochs\n",
    "\n",
    "So we see how the error falls as we run more iterations.\n",
    "It also shows how quickly and how smoothly the model converges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.1714497703163476,\n",
       " 1.1274429632405703,\n",
       " 1.085163382576289,\n",
       " 1.044768479482754,\n",
       " 1.00640039131089]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get error/ loss values for sklearn MLPClassifier (Classification NeuralNetwork)\n",
    "loss_values = nn1.loss_curve_\n",
    "loss_values[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Access the loss_curve_ attribute to retrieve the loss at each iteration\n",
    "loss_values = nn1.loss_curve_\n",
    "\n",
    "# Plot the loss function over epochs\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_values, label='Training Loss')\n",
    "plt.title('Error as a function of the epoch')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (915319529.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [38]\u001b[1;36m\u001b[0m\n\u001b[1;33m    if problem == 'regression':\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Example of how to plot a loss curve if we are using one of the from scratch neural networks\n",
    "\n",
    "# The error values are calculated and accumulated in the backprop function (BP_GD) and returned to the main program\n",
    "problem = 'classification'\n",
    "\n",
    "Er = []\n",
    "\n",
    "if problem == 'regression':\n",
    "    # For each epoch within the gradient descent back prop function (BP_GD)\n",
    "    rmse = np.sqrt(sse/self.TrainData.shape[0]*self.Top[2])\n",
    "    Er = np.append(Er, rmse) # The error for this epoch added to the error list\n",
    "\n",
    "if problem == 'classification':\n",
    "    acc = ........\n",
    "    Er = np.append(Er, acc) # The error for this epoch added to the error list\n",
    "\n",
    "\n",
    "\n",
    "# We we call the BP_GP function we return the value Er within the function into the parameter erEP\n",
    "# erEP is a list that contains the rmse values for each epoch as calcualted within the backprop gradient descent function (BP_GD)\n",
    "erEp,  trainMSE[run] , trainPerf[run] , Epochs[run] = fnn.BP_GD(trainTolerance) \n",
    "\n",
    "# We can then plot the error values for each epoch\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tensor flow\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Fit model : This runs the models and stores the results we want to plot in the history variable\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=4000, verbose=0)\n",
    "\n",
    "# Evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0) # The two method outputs are loss and accuracy (if we define a metric as other than accuracy, then metric fills the second variable)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "\n",
    "# Plot history\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.savefig('nodp.png')\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
