{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are trying to minimize / solve the equation given which was in fact: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x) = x^4 - 3x^3 + 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we use the derivative of that within our gradient descent aglo which is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$4x^3 - 9x^2 = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we apply gradient descent to calculate the 4x^3 - 9x^2 for a value of x\n",
    "\n",
    "- Then we adjust x slightly (in line with the gamma we chose) and calcuate the new value\n",
    "- Then we check if our new value is within our required precision and if so stop\n",
    "-- If not, adjust x again and retry.\n",
    "- Keep doing that until either in our required precision or hitting our max number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial equation given was actually f(x) = x^4 - 3x^3 + 2\n",
    "From that they calculated the derivative of f(x) as being 4x^3 - 9x^2 = 0\n",
    "So it was actually the derivative of f(x), which is f'(x) that we applied gradient descent to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5999999999999996 540\n",
      "1 0.6237599999999996 -2.3759999999999977\n",
      "2 0.6490692731402646 -2.5309273140264934\n",
      "3 0.6760475763767438 -2.6978303236479175\n",
      "4 0.704821965498881 -2.87743891221372\n",
      "5 0.7355261366038248 -3.0704171104943887\n",
      "6 0.7682992721113444 -3.277313550751958\n",
      "7 0.8032842278686305 -3.49849557572861\n",
      "8 0.840624861847519 -3.734063397888848\n",
      "9 0.8804622684298664 -3.9837406582347437\n",
      "10 0.9229296507309586 -4.2467382301092105\n",
      "11 0.9681455460305634 -4.521589529960476\n",
      "12 1.016205130521792 -4.805958449122873\n",
      "13 1.067169389942697 -5.096425942090499\n",
      "14 1.1210520795330405 -5.3882689590343515\n",
      "15 1.1778046421472836 -5.6752562614243125\n",
      "16 1.237299637824332 -5.94949956770484\n",
      "17 1.2993137782331108 -6.20141404087789\n",
      "18 1.3635123370474889 -6.419855881437806\n",
      "19 1.429437442158506 -6.592510511101718\n",
      "20 1.4965033788967752 -6.706593673826919\n",
      "21 1.5640022802344904 -6.749890133771528\n",
      "22 1.6311231270849003 -6.712084685040992\n",
      "23 1.6969855549473505 -6.58624278624502\n",
      "24 1.7606875094969714 -6.370195454962083\n",
      "25 1.821362659674955 -6.067515017798353\n",
      "26 1.8782404675959476 -5.687780792099261\n",
      "27 1.930700009196379 -5.245954160043137\n",
      "28 1.9783089472809865 -4.7608938084607395\n",
      "29 2.0208417065692057 -4.253275928821903\n",
      "30 2.0582751831448975 -3.743347657569167\n",
      "31 2.090764845528107 -3.248966238320982\n",
      "32 2.118607415721545 -2.7842570193437908\n",
      "33 2.1421976265432066 -2.35902108216613\n",
      "34 2.1619858762300224 -1.9788249686815647\n",
      "35 2.178441640823547 -1.645576459352455\n",
      "36 2.1920251576443675 -1.3583516820820591\n",
      "37 2.203167862727841 -1.1142705083473246\n",
      "38 2.2122606942724694 -0.909283154462841\n",
      "39 2.2196486877629633 -0.738799349049394\n",
      "40 2.2256301304909205 -0.5981442727957145\n",
      "41 2.2304587076907274 -0.48285771998069293\n",
      "42 2.234347382687595 -0.3888674996867465\n",
      "43 2.2374730902946083 -0.31257076070134104\n",
      "44 2.239981621916576 -0.2508531621967691\n",
      "45 2.2419923174775156 -0.2010695560939908\n",
      "46 2.243602351591089 -0.16100341135729934\n",
      "47 2.2448905184851697 -0.128816689408076\n",
      "48 2.2459204946033684 -0.10299761181988742\n",
      "49 2.2467436015363202 -0.08231069329518448\n",
      "50 2.2474011148628947 -0.06575133265744171\n",
      "51 2.2479261740485823 -0.05250591856877662\n",
      "52 2.2483453500247714 -0.04191759761889102\n",
      "53 2.2486799240099864 -0.03345739852148455\n",
      "54 2.2489469258218673 -0.02670018118808315\n",
      "55 2.2491599737759116 -0.021304795404432753\n",
      "56 2.2493299520940697 -0.01699783181580017\n",
      "57 2.249465555993498 -0.013560389942867346\n",
      "58 2.24957372949745 -0.010817350395143421\n",
      "59 2.249660016570137 -0.008628707268712787\n",
      "60 2.2497288424102844 -0.006882584014739734\n",
      "61 2.24978373858824 -0.005489617795603863\n",
      "62 2.2498275231061067 -0.0043784517866285455\n",
      "63 2.249862444322635 -0.003492121652847402\n",
      "64 2.249890295941524 -0.0027851618888945495\n",
      "65 2.2499125088471215 -0.0022212905597740473\n",
      "66 2.24993022442776 -0.0017715580638366646\n",
      "67 2.249944353104799 -0.001412867703876941\n",
      "68 2.2499556210437 -0.0011267938901227126\n",
      "69 2.2499646074278457 -0.0008986384145828197\n",
      "Minimum at  2.2499646074278457\n"
     ]
    }
   ],
   "source": [
    "#Source: https: //en.wikipedia.org/wiki/Gradient_descent\n",
    "#Source: https://en.wikipedia.org/w/index.php?title=Gradient_descent&oldid=966271567\n",
    "\n",
    "\n",
    "# Lesson 1: Gradient Descent:\n",
    "# Solving for 4x^3 - 9x^2 = 0\n",
    "\n",
    "next_x = 6# We start the search at x = 6\n",
    "gamma = 0.01# Step size multiplier\n",
    "precision = 0.00001# Desired precision of result\n",
    "max_iters = 10000# Maximum number of iterations\n",
    "\n",
    "# Derivative function : The gradient of the function is computed after differentiating the function with respect to x\n",
    "def df(x):\n",
    "  return 4 * x ** 3 - 9 * x ** 2\n",
    "\n",
    "for i in range(max_iters):\n",
    "    current_x = next_x\n",
    "    next_x = current_x - gamma * df(current_x)\n",
    "    print(i, next_x, df(current_x))\n",
    "\n",
    "    step = next_x - current_x\n",
    "    if abs(step) <= precision:\n",
    "        break\n",
    "\n",
    "print(\"Minimum at \", next_x)\n",
    "\n",
    "# The output for the above will be \"Minimum at 2.2499646074278457\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our result was that the local minimum was 2.499.... with a precision of -0.000896"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
