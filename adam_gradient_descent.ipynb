{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent optimization with adam for a two-dimensional test function\n",
    "from math import sqrt\n",
    "from numpy import asarray\n",
    "from numpy.random import rand\n",
    "from numpy.random import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective function : This is simply an example optimisation problem they created.  Basically saying we are trying to find the minimum of the function x^2 + y^2\n",
    "def objective(x, y):\n",
    "\treturn x**2.0 + y**2.0\n",
    " \n",
    "# derivative of objective function : Given the function we are trying to solve is x^2 + y^2, the derivative of that is 2x + 2y\n",
    "# So we can create an array which contains 2x and 2y\n",
    "def derivative(x, y):\n",
    "\treturn asarray([x * 2.0, y * 2.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This target function (x^2 + y^2) is a paraboloid and has its global minimum at the point (0,0).\n",
    "\n",
    "The derivative function in the script is supposed to calculate the gradient of this objective function. The gradient of a function gives the direction of the steepest ascent. When optimizing, we want to go in the opposite direction (steepest descent) to minimize the function.\n",
    "\n",
    "For the given quadratic function, the gradient can be calculated analytically by taking the partial derivatives with respect to each variable (x and y). Here's how it works:\n",
    "\n",
    "    The derivative of x^2 with respect to x is 2x.\n",
    "    The derivative of y^2 with respect to y is 2y.\n",
    "\n",
    "Therefore, the gradient of the function f(x,y)f(x,y) is a vector consisting of these partial derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">0 f([-0.14595599  0.42064899]) = 0.19825\n",
      ">1 f([-0.12613855  0.40070573]) = 0.17648\n",
      ">2 f([-0.10665938  0.3808601 ]) = 0.15643\n",
      ">3 f([-0.08770234  0.3611548 ]) = 0.13812\n",
      ">4 f([-0.06947941  0.34163405]) = 0.12154\n",
      ">5 f([-0.05222756  0.32234308]) = 0.10663\n",
      ">6 f([-0.03620086  0.30332769]) = 0.09332\n",
      ">7 f([-0.02165679  0.28463383]) = 0.08149\n",
      ">8 f([-0.00883663  0.26630707]) = 0.07100\n",
      ">9 f([0.00205801 0.24839209]) = 0.06170\n",
      ">10 f([0.01088844 0.23093228]) = 0.05345\n",
      ">11 f([0.01759677 0.2139692 ]) = 0.04609\n",
      ">12 f([0.02221425 0.19754214]) = 0.03952\n",
      ">13 f([0.02485859 0.18168769]) = 0.03363\n",
      ">14 f([0.02572196 0.16643933]) = 0.02836\n",
      ">15 f([0.02505339 0.15182705]) = 0.02368\n",
      ">16 f([0.02313917 0.13787701]) = 0.01955\n",
      ">17 f([0.02028406 0.12461125]) = 0.01594\n",
      ">18 f([0.01679451 0.11204744]) = 0.01284\n",
      ">19 f([0.01296436 0.10019867]) = 0.01021\n",
      ">20 f([0.00906264 0.08907337]) = 0.00802\n",
      ">21 f([0.00532366 0.07867522]) = 0.00622\n",
      ">22 f([0.00193919 0.06900318]) = 0.00477\n",
      ">23 f([-0.00094677  0.06005154]) = 0.00361\n",
      ">24 f([-0.00324034  0.05181012]) = 0.00269\n",
      ">25 f([-0.00489722  0.04426444]) = 0.00198\n",
      ">26 f([-0.00591902  0.03739604]) = 0.00143\n",
      ">27 f([-0.00634719  0.0311828 ]) = 0.00101\n",
      ">28 f([-0.00625503  0.02559933]) = 0.00069\n",
      ">29 f([-0.00573849  0.02061737]) = 0.00046\n",
      ">30 f([-0.00490679  0.01620624]) = 0.00029\n",
      ">31 f([-0.00387317  0.01233332]) = 0.00017\n",
      ">32 f([-0.00274675  0.00896449]) = 0.00009\n",
      ">33 f([-0.00162559  0.00606458]) = 0.00004\n",
      ">34 f([-0.00059149  0.00359785]) = 0.00001\n",
      ">35 f([0.0002934  0.00152838]) = 0.00000\n",
      ">36 f([ 0.00098821 -0.00017954]) = 0.00000\n",
      ">37 f([ 0.00147307 -0.00156101]) = 0.00000\n",
      ">38 f([ 0.00174746 -0.00265025]) = 0.00001\n",
      ">39 f([ 0.00182724 -0.00348028]) = 0.00002\n",
      ">40 f([ 0.00174089 -0.00408267]) = 0.00002\n",
      ">41 f([ 0.00152536 -0.00448737]) = 0.00002\n",
      ">42 f([ 0.00122173 -0.00472254]) = 0.00002\n",
      ">43 f([ 0.00087133 -0.00481438]) = 0.00002\n",
      ">44 f([ 0.00051228 -0.00478712]) = 0.00002\n",
      ">45 f([ 0.00017692 -0.00466292]) = 0.00002\n",
      ">46 f([-0.00011001 -0.00446188]) = 0.00002\n",
      ">47 f([-0.00033219 -0.004202  ]) = 0.00002\n",
      ">48 f([-0.00048176 -0.00389929]) = 0.00002\n",
      ">49 f([-0.00055861 -0.00356777]) = 0.00001\n",
      ">50 f([-0.00056912 -0.00321961]) = 0.00001\n",
      ">51 f([-0.00052452 -0.00286514]) = 0.00001\n",
      ">52 f([-0.00043908 -0.00251304]) = 0.00001\n",
      ">53 f([-0.0003283  -0.00217044]) = 0.00000\n",
      ">54 f([-0.00020731 -0.00184302]) = 0.00000\n",
      ">55 f([-8.95352320e-05 -1.53514076e-03]) = 0.00000\n",
      ">56 f([ 1.43050285e-05 -1.25002847e-03]) = 0.00000\n",
      ">57 f([ 9.67123406e-05 -9.89850279e-04]) = 0.00000\n",
      ">58 f([ 0.00015359 -0.00075587]) = 0.00000\n",
      ">59 f([ 0.00018407 -0.00054858]) = 0.00000\n",
      "Done!\n",
      "f([ 0.00018407 -0.00054858]) = 0.000000\n"
     ]
    }
   ],
   "source": [
    "# gradient descent algorithm with adam\n",
    "def adam(objective, derivative, bounds, n_iter, alpha, beta1, beta2, eps=1e-8):\n",
    "\t# generate an initial point\n",
    "\tx = bounds[:, 0] + rand(len(bounds)) * (bounds[:, 1] - bounds[:, 0])\n",
    "\tscore = objective(x[0], x[1])\n",
    "\t# initialize first and second moments\n",
    "\tm = [0.0 for _ in range(bounds.shape[0])]\n",
    "\tv = [0.0 for _ in range(bounds.shape[0])]\n",
    "\t# run the gradient descent updates\n",
    "\tfor t in range(n_iter):\n",
    "\t\t# calculate gradient g(t)\n",
    "\t\tg = derivative(x[0], x[1])\n",
    "\t\t# build a solution one variable at a time\n",
    "\t\tfor i in range(x.shape[0]):\n",
    "\t\t\t# m(t) = beta1 * m(t-1) + (1 - beta1) * g(t)\n",
    "\t\t\tm[i] = beta1 * m[i] + (1.0 - beta1) * g[i]\n",
    "\t\t\t# v(t) = beta2 * v(t-1) + (1 - beta2) * g(t)^2\n",
    "\t\t\tv[i] = beta2 * v[i] + (1.0 - beta2) * g[i]**2\n",
    "\t\t\t# mhat(t) = m(t) / (1 - beta1(t))\n",
    "\t\t\tmhat = m[i] / (1.0 - beta1**(t+1))\n",
    "\t\t\t# vhat(t) = v(t) / (1 - beta2(t))\n",
    "\t\t\tvhat = v[i] / (1.0 - beta2**(t+1))\n",
    "\t\t\t# x(t) = x(t-1) - alpha * mhat(t) / (sqrt(vhat(t)) + eps)\n",
    "\n",
    "\t\t\t## This is the actual update step, updating the parameter being used to minimize the loss\n",
    "\t\t\t## In this case, we are updating the x and y values\n",
    "\t\t\t## But if we using it in a neural network, we would be updating the weights and biases\n",
    "\t\t\tx[i] = x[i] - alpha * mhat / (sqrt(vhat) + eps)\n",
    "\t\t# evaluate candidate point\n",
    "\t\tscore = objective(x[0], x[1])\n",
    "\t\t# report progress\n",
    "\t\tprint('>%d f(%s) = %.5f' % (t, x, score))\n",
    "\treturn [x, score]\n",
    " \n",
    "# seed the pseudo random number generator\n",
    "seed(1)\n",
    "#source: https://machinelearningmastery.com/adam-optimization-from-scratch/\n",
    "\n",
    "# define range for input\n",
    "bounds = asarray([[-1.0, 1.0], [-1.0, 1.0]])\n",
    "# define the total iterations\n",
    "n_iter = 60\n",
    "# steps size\n",
    "alpha = 0.02\n",
    "# factor for average gradient\n",
    "beta1 = 0.8\n",
    "# factor for average squared gradient\n",
    "beta2 = 0.999\n",
    "# perform the gradient descent search with adam\n",
    "best, score = adam(objective, derivative, bounds, n_iter, alpha, beta1, beta2)\n",
    "print('Done!')\n",
    "print('f(%s) = %f' % (best, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROGRAM EXPLANATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script Summary:\n",
    "The script is using the Adam optimization algorithm to find the minimum of a quadratic function, which is simply the sum of the squares of two variables, x and y. The algorithm iteratively adjusts the values of x and y to minimize this function.\n",
    "\n",
    "Here's a step-by-step explanation of how the script operates:\n",
    "\n",
    "    Objective Function (objective): This function computes the value of the quadratic function for given x and y.\n",
    "\n",
    "    Derivative Function (derivative): This function calculates the gradient of the objective function, which are the partial derivatives with respect to x and y.\n",
    "\n",
    "    Adam Optimization Function (adam):\n",
    "        Starts by generating a random initial point within specified bounds.\n",
    "        Initializes the first (m) and second (v) moment vectors, which are used for computing bias-corrected estimates of the gradient and its square, respectively.\n",
    "        Performs gradient descent updates for a specified number of iterations (n_iter):\n",
    "            Computes the gradient of the objective function at the current point.\n",
    "            Updates the moment vectors m and v.\n",
    "            Calculates bias-corrected estimates of the moments (mhat and vhat).\n",
    "            Updates the variables x and y by moving in the direction that reduces the objective function, considering the learning rate (alpha), the corrected moments, and a small number eps to prevent division by zero.\n",
    "            Evaluates and prints the current objective function value.\n",
    "        Returns the best solution found and the corresponding objective function value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VARIABLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables:\n",
    "\n",
    "    x: A NumPy array representing the current point (containing x and y values).\n",
    "    score: The current value of the objective function at point x.\n",
    "    m: A list representing the first moment vector (related to the moving average of the gradients).\n",
    "    v: A list representing the second moment vector (related to the moving average of the squared gradients).\n",
    "    g: The gradient of the objective function at point x.\n",
    "    mhat: Bias-corrected estimate of the first moment vector.\n",
    "    vhat: Bias-corrected estimate of the second moment vector.\n",
    "    alpha (step size): The learning rate controlling the step size in the parameter update.\n",
    "    beta1: The decay rate for the first moment estimates (similar to the momentum factor).\n",
    "    beta2: The decay rate for the second moment estimates.\n",
    "    eps: A small constant to prevent division by zero in the parameter update.\n",
    "    bounds: The bounds for the initial point values.\n",
    "    n_iter: The number of iterations for which the optimization will run.\n",
    "    best: The best point found by the algorithm.\n",
    "    seed(1): This seeds the random number generator to ensure reproducible results.\n",
    "\n",
    "The main purpose of these variables is to define the optimization problem, control the behavior of the Adam optimizer, and store the necessary information for each iteration, like the history and magnitude of the gradients."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
