{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Dataset\n",
      "     outlook  temp humidity  windy play\n",
      "0   overcast   hot     high  FALSE  yes\n",
      "1   overcast  cool   normal   TRUE  yes\n",
      "2   overcast  mild     high   TRUE  yes\n",
      "3   overcast   hot   normal  FALSE  yes\n",
      "4      rainy  mild     high  FALSE  yes\n",
      "5      rainy  cool   normal  FALSE  yes\n",
      "6      rainy  cool   normal   TRUE   no\n",
      "7      rainy  mild   normal  FALSE  yes\n",
      "8      rainy  mild     high   TRUE   no\n",
      "9      sunny   hot     high  FALSE   no\n",
      "10     sunny   hot     high   TRUE   no\n",
      "11     sunny  mild     high  FALSE   no\n",
      "12     sunny  cool   normal  FALSE  yes\n",
      "13     sunny  mild   normal   TRUE  yes\n",
      "Index(['outlook', 'temp', 'humidity', 'windy', 'play'], dtype='object')  list of attributes\n",
      "play  is Class\n",
      "\n",
      "VALUE:  yes\n",
      "Entropy for yes is: 0.40977637753840185\n",
      "The count and fraction of each unique value in the target variable, along with the cumulative entropy after considering each value.\n",
      "df[Class].value_counts()[value]=9, fraction=0.6428571428571429, entropy=0.40977637753840185, value='yes'\n",
      "\n",
      "VALUE:  no\n",
      "Entropy for no is: 0.5305095811322292\n",
      "The count and fraction of each unique value in the target variable, along with the cumulative entropy after considering each value.\n",
      "df[Class].value_counts()[value]=5, fraction=0.35714285714285715, entropy=0.9402859586706311, value='no'\n",
      "\n",
      "Final Entropy Calculation for the Whole Dataset:  0.9402859586706311\n"
     ]
    }
   ],
   "source": [
    "#source: https://medium.com/@lope.ai/decision-trees-from-scratch-using-id3-python-coding-it-up-6b79e3458de4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#eps = np.finfo(float).eps # This initiates a very small number to prevent the log function to become zero\n",
    "from numpy import log2 as log #log2 is log with base 2\n",
    "\n",
    "\n",
    "def find_entropy(df, factor):\n",
    "    \"\"\"\n",
    "    Calculates the entropy of a given DataFrame.\n",
    "    It identifies the target variable (class) as the last column in the DataFrame.\n",
    "    It calculates entropy using the formula for Shannon entropy, which is -Σ(p(x) * log₂(p(x))), where p(x) is the fraction of the number of elements in class x to the total number of elements.\n",
    "    It prints out the count and fraction of each unique value in the target variable, along with the cumulative entropy after considering each value.\n",
    "    \"\"\"\n",
    "\n",
    "    #Class = df.keys()[-1]   #To make the code generic, changing target variable class name\n",
    "    Class = factor   #To make the code generic, allowing factor to be fed into function\n",
    "    print(df.keys(), ' list of attributes')\n",
    "    print(Class, ' is Class')\n",
    "    entropy = 0\n",
    "    values = df[Class].unique()\n",
    "    for value in values:\n",
    "        print('\\nVALUE: ', value)\n",
    "        fraction = df[Class].value_counts()[value]/len(df[Class]) #this line is the fraction of the number of elements in class x to the total number of elements.\n",
    "\n",
    "        print(f\"Entropy for {value} is: {-fraction*np.log2(fraction)}\")\n",
    "        entropy += -fraction*np.log2(fraction) # this line is the Shannon entropy formula\n",
    "\n",
    "        print('The count and fraction of each unique value in the target variable, along with the cumulative entropy after considering each value.')\n",
    "        print(f\"{df[Class].value_counts()[value]=}, {fraction=}, {entropy=}, {value=}\") \n",
    "        # prints out the count and fraction of each unique value in the target variable, along with the cumulative entropy after considering each value.\n",
    "    return entropy\n",
    "\n",
    "\n",
    "outlook = 'overcast,overcast,overcast,overcast,rainy,rainy,rainy,rainy,rainy,sunny,sunny,sunny,sunny,sunny'.split(',')\n",
    "temp = 'hot,cool,mild,hot,mild,cool,cool,mild,mild,hot,hot,mild,cool,mild'.split(',')\n",
    "humidity = 'high,normal,high,normal,high,normal,normal,normal,high,high,high,high,normal,normal'.split(',')\n",
    "windy = 'FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE'.split(',')\n",
    "play = 'yes,yes,yes,yes,yes,yes,no,yes,no,no,no,no,yes,yes'.split(',')\n",
    "\n",
    "dataset ={'outlook':outlook,'temp':temp,'humidity':humidity,'windy':windy,'play':play}\n",
    "df = pd.DataFrame(dataset,columns=['outlook','temp','humidity','windy','play'])\n",
    "\n",
    "print(\"Initial Dataset\")\n",
    "print(df)\n",
    "\n",
    "# Run the entropy calculation function for the specified factor\n",
    "# Running for play, the target variable gives us a value for the whole dataset\n",
    "print(\"\\nFinal Entropy Calculation for the Whole Dataset: \", find_entropy(df , 'play'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy for Wind Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     outlook  temp humidity  windy play\n",
      "0   overcast   hot     high  FALSE  yes\n",
      "1   overcast  cool   normal   TRUE  yes\n",
      "2   overcast  mild     high   TRUE  yes\n",
      "3   overcast   hot   normal  FALSE  yes\n",
      "4      rainy  mild     high  FALSE  yes\n",
      "5      rainy  cool   normal  FALSE  yes\n",
      "6      rainy  cool   normal   TRUE   no\n",
      "7      rainy  mild   normal  FALSE  yes\n",
      "8      rainy  mild     high   TRUE   no\n",
      "9      sunny   hot     high  FALSE   no\n",
      "10     sunny   hot     high   TRUE   no\n",
      "11     sunny  mild     high  FALSE   no\n",
      "12     sunny  cool   normal  FALSE  yes\n",
      "13     sunny  mild   normal   TRUE  yes\n",
      "Index(['outlook', 'temp', 'humidity', 'windy', 'play'], dtype='object')  list of attributes\n",
      "windy  is Class\n",
      "\n",
      "VALUE:  FALSE\n",
      "Entropy for FALSE is: 0.46134566974720237\n",
      "The count and fraction of each unique value in the target variable, along with the cumulative entropy after considering each value.\n",
      "df[Class].value_counts()[value]=8, fraction=0.5714285714285714, entropy=0.46134566974720237, value='FALSE'\n",
      "\n",
      "VALUE:  TRUE\n",
      "Entropy for TRUE is: 0.5238824662870492\n",
      "The count and fraction of each unique value in the target variable, along with the cumulative entropy after considering each value.\n",
      "df[Class].value_counts()[value]=6, fraction=0.42857142857142855, entropy=0.9852281360342515, value='TRUE'\n",
      "\n",
      "Final Entropy Calculation for the Whole Dataset:  0.9852281360342515\n"
     ]
    }
   ],
   "source": [
    "\n",
    "outlook = 'overcast,overcast,overcast,overcast,rainy,rainy,rainy,rainy,rainy,sunny,sunny,sunny,sunny,sunny'.split(',')\n",
    "temp = 'hot,cool,mild,hot,mild,cool,cool,mild,mild,hot,hot,mild,cool,mild'.split(',')\n",
    "humidity = 'high,normal,high,normal,high,normal,normal,normal,high,high,high,high,normal,normal'.split(',')\n",
    "windy = 'FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE'.split(',')\n",
    "play = 'yes,yes,yes,yes,yes,yes,no,yes,no,no,no,no,yes,yes'.split(',')\n",
    "\n",
    "dataset ={'outlook':outlook,'temp':temp,'humidity':humidity,'windy':windy,'play':play}\n",
    "df = pd.DataFrame(dataset,columns=['outlook','temp','humidity','windy','play'])\n",
    "\n",
    "print(df)\n",
    "\n",
    "print(\"\\nFinal Entropy Calculation for the Whole Dataset: \", find_entropy(df , 'windy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Entropy: For each unique value, it calculates \n",
    "\n",
    "    −p(xi) * log⁡2p(xi)  \n",
    "                        \n",
    " where p(xi) is the proportion of number of elements in the class xi to the total number of elements (the percentage representation)\n",
    "and sums these values for the dataset to get the total entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy Calc for any Given Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROGRAM DESCRIPTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided Python code is an implementation of a Decision Tree using the ID3 algorithm. Let's break down the code step by step:\n",
    "\n",
    "    Import Libraries and Initialization: The code imports necessary libraries, numpy and pandas, and initializes a small value eps using the machine's precision for floating-point numbers. This value is not used in the code, though.\n",
    "\n",
    "    Definition of find_entropy Function:\n",
    "        The function find_entropy calculates the entropy of a given DataFrame.\n",
    "        It identifies the target variable (class) as the last column in the DataFrame.\n",
    "        It calculates entropy using the formula for Shannon entropy, which is -Σ(p(x) * log₂(p(x))), where p(x) is the fraction of the number of elements in class x to the total number of elements.\n",
    "        It prints out the count and fraction of each unique value in the target variable, along with the cumulative entropy after considering each value.\n",
    "\n",
    "    Dataset Preparation:\n",
    "        The code constructs a dataset using lists of values for weather conditions ('outlook', 'temp', 'humidity', 'windy') and the target variable 'play'.\n",
    "        These lists are combined into a dictionary with keys corresponding to column names and values to the list of column values.\n",
    "        A DataFrame df is created from this dictionary using pandas.\n",
    "\n",
    "    Print Statements:\n",
    "        The code prints the DataFrame df.\n",
    "        It then calls the find_entropy function on this DataFrame and prints the resulting entropy.\n",
    "\n",
    "In summary, this code demonstrates how to calculate the entropy of a dataset, which is a crucial step in building a Decision Tree classifier using the ID3 algorithm. The dataset used here seems to be related to determining whether to play or not based on weather conditions. The entropy calculation is a measure of the impurity in the dataset, which helps in deciding which attribute to split on at each step in the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
