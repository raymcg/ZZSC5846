{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix as CM\n",
    "\n",
    "\n",
    "def I(flag):\n",
    "    return 1 if flag else 0\n",
    "\n",
    "def sign(x):\n",
    "    return abs(x)/x if x!=0 else 1       \n",
    "\n",
    "class AdaBoost:\n",
    "    \n",
    "    def __init__(self,n_estimators=50):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.models = [None]*n_estimators\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        \n",
    "        X = np.float64(X)\n",
    "        N = len(y)\n",
    "        print(\"Number of Training Rows: \", N)\n",
    "        w = np.array([1/N for i in range(N)])\n",
    "        print('Initial weights: ', w)\n",
    "        print()\n",
    "        \n",
    "        for m in range(self.n_estimators):\n",
    "      \n",
    "            # This line builds the stump to be tested\n",
    "            # The DecisionTreeClassifier library is doing the heavy lifting to work out which feature to split on and how to split that feature.\n",
    "            Gm = DecisionTreeClassifier(max_depth=1).fit(X,y,sample_weight=w).predict\n",
    "                        \n",
    "            errM = sum([w[i]*I(y[i]!=Gm(X[i].reshape(1,-1))) for i in range(N)])/sum(w) # error for this model/ iteration\n",
    " \n",
    "            AlphaM = np.log((1-errM)/errM) # alpha for this model/ iteration\n",
    "\n",
    "            w = [w[i]*np.exp(AlphaM*I(y[i]!=Gm(X[i].reshape(1,-1)))) for i in range(N)] # update weights\n",
    "            \n",
    "            #print(\"M = \", m, \"of \", self.n_estimators)\n",
    "            print(\"M = \", m, \"of \", self.n_estimators, '. Alpha for this m:', AlphaM)\n",
    "            print(w, ' w')\n",
    "            print(Gm, ' Gm')\n",
    "            print(errM, ' errM')\n",
    "            \n",
    "            self.models[m] = (AlphaM,Gm)\n",
    "\n",
    "    def predict(self,X):\n",
    "        \n",
    "        y = 0\n",
    "        for m in range(self.n_estimators):\n",
    "            AlphaM,Gm = self.models[m]\n",
    "\n",
    "            y += AlphaM*Gm(X)\n",
    "            #print(y, m, ' y m ')\n",
    "        print(y, ' y *')\n",
    "        signA = np.vectorize(sign)\n",
    "        y = np.where(signA(y)==-1,-1,1)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training Rows:  25\n",
      "Initial weights:  [0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04\n",
      " 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04]\n",
      "\n",
      "M =  0 of  5 . Alpha for this m: 1.9924301646902065\n",
      "[0.29333333333333345, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.29333333333333345, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.29333333333333345]  w\n",
      "<bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=1)>  Gm\n",
      "0.11999999999999997  errM\n",
      "M =  1 of  5 . Alpha for this m: 2.0541237336955462\n",
      "[0.29333333333333345, 0.04, 0.04, 0.04, 0.04, 0.31200000000000006, 0.04, 0.04, 0.04, 0.04, 0.31200000000000006, 0.04, 0.04, 0.04, 0.04, 0.31200000000000006, 0.29333333333333345, 0.04, 0.04, 0.04, 0.04, 0.04, 0.31200000000000006, 0.31200000000000006, 0.29333333333333345]  w\n",
      "<bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=1)>  Gm\n",
      "0.1136363636363636  errM\n",
      "M =  2 of  5 . Alpha for this m: 1.9169226121820615\n",
      "[0.29333333333333345, 0.04, 0.27200000000000013, 0.27200000000000013, 0.27200000000000013, 0.31200000000000006, 0.04, 0.27200000000000013, 0.27200000000000013, 0.27200000000000013, 0.31200000000000006, 0.04, 0.04, 0.27200000000000013, 0.27200000000000013, 0.31200000000000006, 0.29333333333333345, 0.04, 0.04, 0.27200000000000013, 0.04, 0.27200000000000013, 0.31200000000000006, 0.31200000000000006, 0.29333333333333345]  w\n",
      "<bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=1)>  Gm\n",
      "0.12820512820512814  errM\n",
      "M =  3 of  5 . Alpha for this m: 1.6451559950361798\n",
      "[1.520000000000001, 0.04, 0.27200000000000013, 0.27200000000000013, 0.27200000000000013, 0.31200000000000006, 0.04, 0.27200000000000013, 0.27200000000000013, 0.27200000000000013, 0.31200000000000006, 0.04, 0.04, 0.27200000000000013, 0.27200000000000013, 0.31200000000000006, 1.520000000000001, 0.04, 0.04, 0.27200000000000013, 0.04, 0.27200000000000013, 0.31200000000000006, 0.31200000000000006, 1.520000000000001]  w\n",
      "<bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=1)>  Gm\n",
      "0.16176470588235292  errM\n",
      "M =  4 of  5 . Alpha for this m: 1.769423515285992\n",
      "[1.520000000000001, 0.04, 0.27200000000000013, 0.27200000000000013, 0.27200000000000013, 0.31200000000000006, 0.04, 1.5959518072289167, 1.5959518072289167, 1.5959518072289167, 0.31200000000000006, 0.23469879518072292, 0.23469879518072292, 0.27200000000000013, 0.27200000000000013, 1.8306506024096392, 1.520000000000001, 0.23469879518072292, 0.23469879518072292, 0.27200000000000013, 0.23469879518072292, 0.27200000000000013, 0.31200000000000006, 0.31200000000000006, 1.520000000000001]  w\n",
      "<bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=1)>  Gm\n",
      "0.14561403508771928  errM\n",
      " predict ...\n",
      "[-2.1028837   9.37805602  5.5442108  -5.5442108  -5.5442108   5.26980855\n",
      "  9.37805602 -2.00536377 -2.00536377 -2.00536377  5.26980855 -5.83920899\n",
      " -5.83920899 -5.5442108  -5.5442108   1.73096152 -2.1028837   5.83920899\n",
      "  5.83920899  5.5442108   5.83920899 -5.5442108   5.26980855  5.26980855\n",
      " -2.1028837 ]  y *\n"
     ]
    }
   ],
   "source": [
    "x,y = make_classification(n_samples=25)\n",
    "\n",
    "#x = x[:,0:5] # reducing the feature space \n",
    "\n",
    "#print(x,y, ' data')\n",
    "\n",
    "'''\n",
    "As for our implementaion of AdaBoost \n",
    "y needs to be in {-1,1}\n",
    "'''\n",
    "y = np.where(y==0,-1,1)\n",
    "\n",
    "clf = AdaBoost(n_estimators=5) # try 5 10 50 and press Run over and over again\n",
    "clf.fit(x,y)\n",
    "print(' predict ...')\n",
    "\n",
    "# Run the prediction function defined above\n",
    "y_pred = clf.predict(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  1  1 -1 -1  1  1 -1 -1 -1  1 -1 -1 -1 -1  1 -1  1  1  1  1 -1  1  1\n",
      " -1]  predicted\n"
     ]
    }
   ],
   "source": [
    "print(y_pred, ' predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: 100.0\n",
      "Confusion Matrix: [[13  0]\n",
      " [ 0 12]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance:\",100*sum(y_pred==y)/len(y))\n",
    "print(\"Confusion Matrix:\",CM(y,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
